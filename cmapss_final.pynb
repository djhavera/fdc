{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb14d52",
   "metadata": {},
   "source": [
    "# CS598 FDC Final Project: C-MAPSS Turbofan Degradation Data Curation Pipeline\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This notebook implements a comprehensive data curation pipeline for NASA's **C-MAPSS (Commercial Modular Aero-Propulsion System Simulation)** turbofan engine degradation dataset. The project demonstrates end-to-end application of data curation principles through the **USGS Science Data Lifecycle Model (SDLM)** framework, transforming raw sensor data into analysis-ready datasets suitable for predictive maintenance research.\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "- **Source**: NASA Prognostics Center of Excellence\n",
    "- **Domain**: Aircraft turbofan engine health monitoring\n",
    "- **Scale**: 1,008 engine units across 4 sub-datasets (FD001-FD004)\n",
    "- **Observations**: 132,332 time-series records\n",
    "- **Features**: 26 sensor measurements + 3 operational settings + metadata\n",
    "- **Use Case**: Remaining Useful Life (RUL) prediction for condition-based maintenance\n",
    "\n",
    "**Data Architecture**: Kimball Star Schema with embedded dimensions, optimized for time-series analysis and machine learning workflows.\n",
    "\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| Output | Description |  \n",
    "|--------|-------------|\n",
    "| **fact_table** | Raw fact table (132,332 rows × 32 columns) | \n",
    "| **fact_table_clean** | Outliers removed (131,332 rows × 32 columns) | \n",
    "| **fact_table_transformed** | Normalized + engineered features (131,332 rows × 36 columns) | \n",
    "\n",
    "### Course Requirements Coverage & Cross-References\n",
    "\n",
    "| Module | Requirement | Notebook Section | PDF Report Section |\n",
    "|--------|-------------|-----------------|-------------------|\n",
    "| **M1** | Data lifecycle models | Entire notebook (SDLM framework) | Sections 1-7 |\n",
    "| **M2** | Ethical, legal, policy constraints | Section 2.3.1 | Section 1.2 |\n",
    "| **M3-M5** | Data models & abstractions | Section 2.3.8 | Section 4.1 |\n",
    "| **M6** | Data cleaning (quality assurance) | Section 3.1, 3.2 | Section 3.1.2, 3.1.3 |\n",
    "| **M7** | What is data? | N/A (covered in report) | Section 7.1.1 |\n",
    "| **M8** | Metadata & documentation | Section 2.3.1 | Section 3.1.1 |\n",
    "| **M9** | Identity & identifier systems | Section 2.3.2 | Section 5 |\n",
    "| **M10** | Preservation | N/A (covered in report) | Section 5 |\n",
    "| **M11** | Standards & best practices | Section 2.3.3 | Section 7.1.2 |\n",
    "| **M12** | Workflow automation & reproducibility | Section 2.3.7 | Section 5 |\n",
    "| **M13** | Data practices (sharing, reuse, reproducibility) | Section 2.3.5 | Section 5 |\n",
    "| **M15** | Dissemination & communication | Section 3.3 | Section 6 |\n",
    "\n",
    "**Dependencies**: See `requirements.txt` for Python package versions.\n",
    "\n",
    "**Data Source**: NASA Prognostics Center of Excellence - C-MAPSS Dataset  \n",
    "**Citation**: Saxena, A., & Goebel, K. (2008). *Turbofan Engine Degradation Simulation Data Set*. NASA Ames Research Center.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e724e7fb",
   "metadata": {},
   "source": [
    "## 1. Set Up Python Environment and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f489d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n",
      "Data directory: /Users/davidhavera/CS598_FDC/fact_table\n",
      "Output directory: /Users/davidhavera/CS598_FDC/processed_data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('default')\n",
    "\n",
    "# Define data directory path\n",
    "DATA_DIR = Path('/Users/davidhavera/CS598_FDC/fact_table')\n",
    "OUTPUT_DIR = Path('/Users/davidhavera/CS598_FDC/processed_data')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0621b023",
   "metadata": {},
   "source": [
    "## 2.1 Load and Parse C-MAPSS Raw Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b9cc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 11:24:16,517 - INFO - Loaded train_FD001.txt: 20631 rows, 26 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C-MAPSS datasets...\n",
      "train_FD001: (20631, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 11:24:16,707 - INFO - Loaded train_FD002.txt: 53759 rows, 26 columns\n",
      "2025-10-27 11:24:16,824 - INFO - Loaded train_FD003.txt: 24720 rows, 26 columns\n",
      "2025-10-27 11:24:16,824 - INFO - Loaded train_FD003.txt: 24720 rows, 26 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_FD002: (53759, 26)\n",
      "train_FD003: (24720, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 11:24:17,005 - INFO - Loaded train_FD004.txt: 61249 rows, 26 columns\n",
      "2025-10-27 11:24:17,050 - INFO - Loaded test_FD001.txt: 13096 rows, 26 columns\n",
      "2025-10-27 11:24:17,050 - INFO - Loaded test_FD001.txt: 13096 rows, 26 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_FD004: (61249, 26)\n",
      "test_FD001: (13096, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 11:24:17,249 - INFO - Loaded test_FD002.txt: 33991 rows, 26 columns\n",
      "2025-10-27 11:24:17,314 - INFO - Loaded test_FD003.txt: 16596 rows, 26 columns\n",
      "2025-10-27 11:24:17,314 - INFO - Loaded test_FD003.txt: 16596 rows, 26 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_FD002: (33991, 26)\n",
      "test_FD003: (16596, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 11:24:17,470 - INFO - Loaded test_FD004.txt: 41214 rows, 26 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_FD004: (41214, 26)\n",
      "\n",
      "Successfully loaded 8 datasets\n"
     ]
    }
   ],
   "source": [
    "# Define column names for the C-MAPSS dataset (26 columns total)\n",
    "COLUMN_NAMES = [\n",
    "    'unit_id',          # Engine unit identifier\n",
    "    'time_cycles',      # Time in cycles\n",
    "    'op_setting_1',     # Operational setting 1\n",
    "    'op_setting_2',     # Operational setting 2  \n",
    "    'op_setting_3',     # Operational setting 3\n",
    "    'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5',\n",
    "    'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10',\n",
    "    'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15',\n",
    "    'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20',\n",
    "    'sensor_21'\n",
    "]\n",
    "\n",
    "# Load C-MAPSS data from space-separated text file.\n",
    "def load_cmapss_data(file_path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        # Read space-separated data without header\n",
    "        df = pd.read_csv(file_path, sep='\\s+', header=None, names=COLUMN_NAMES)\n",
    "        logger.info(f\"Loaded {file_path.name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load all training and test datasets\n",
    "datasets = {}\n",
    "dataset_files = {\n",
    "    'train_FD001': 'train_FD001.txt',\n",
    "    'train_FD002': 'train_FD002.txt', \n",
    "    'train_FD003': 'train_FD003.txt',\n",
    "    'train_FD004': 'train_FD004.txt',\n",
    "    'test_FD001': 'test_FD001.txt',\n",
    "    'test_FD002': 'test_FD002.txt',\n",
    "    'test_FD003': 'test_FD003.txt',\n",
    "    'test_FD004': 'test_FD004.txt'\n",
    "}\n",
    "\n",
    "print(\"Loading C-MAPSS datasets...\")\n",
    "for dataset_name, filename in dataset_files.items():\n",
    "    file_path = DATA_DIR / filename\n",
    "    if file_path.exists():\n",
    "        datasets[dataset_name] = load_cmapss_data(file_path)\n",
    "        print(f\"{dataset_name}: {datasets[dataset_name].shape}\")\n",
    "    else:\n",
    "        print(f\"{dataset_name}: File not found\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "955d69fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading RUL target values...\n",
      "RUL_FD001: 100 target values\n",
      "RUL_FD002: 259 target values\n",
      "RUL_FD003: 100 target values\n",
      "RUL_FD004: 248 target values\n",
      "\n",
      "==================================================\n",
      "Sample of train_FD001 dataset:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_id</th>\n",
       "      <th>time_cycles</th>\n",
       "      <th>op_setting_1</th>\n",
       "      <th>op_setting_2</th>\n",
       "      <th>op_setting_3</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "      <th>sensor_13</th>\n",
       "      <th>sensor_14</th>\n",
       "      <th>sensor_15</th>\n",
       "      <th>sensor_16</th>\n",
       "      <th>sensor_17</th>\n",
       "      <th>sensor_18</th>\n",
       "      <th>sensor_19</th>\n",
       "      <th>sensor_20</th>\n",
       "      <th>sensor_21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.36</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9046.19</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.47</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>553.75</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>9044.07</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.49</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.26</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>9052.94</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.27</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.45</td>\n",
       "      <td>2388.11</td>\n",
       "      <td>9049.48</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.13</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.00</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9055.15</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.28</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unit_id  time_cycles  op_setting_1  op_setting_2  op_setting_3  sensor_1  \\\n",
       "0        1            1       -0.0007       -0.0004         100.0    518.67   \n",
       "1        1            2        0.0019       -0.0003         100.0    518.67   \n",
       "2        1            3       -0.0043        0.0003         100.0    518.67   \n",
       "3        1            4        0.0007        0.0000         100.0    518.67   \n",
       "4        1            5       -0.0019       -0.0002         100.0    518.67   \n",
       "\n",
       "   sensor_2  sensor_3  sensor_4  sensor_5  sensor_6  sensor_7  sensor_8  \\\n",
       "0    641.82   1589.70   1400.60     14.62     21.61    554.36   2388.06   \n",
       "1    642.15   1591.82   1403.14     14.62     21.61    553.75   2388.04   \n",
       "2    642.35   1587.99   1404.20     14.62     21.61    554.26   2388.08   \n",
       "3    642.35   1582.79   1401.87     14.62     21.61    554.45   2388.11   \n",
       "4    642.37   1582.85   1406.22     14.62     21.61    554.00   2388.06   \n",
       "\n",
       "   sensor_9  sensor_10  sensor_11  sensor_12  sensor_13  sensor_14  sensor_15  \\\n",
       "0   9046.19        1.3      47.47     521.66    2388.02    8138.62     8.4195   \n",
       "1   9044.07        1.3      47.49     522.28    2388.07    8131.49     8.4318   \n",
       "2   9052.94        1.3      47.27     522.42    2388.03    8133.23     8.4178   \n",
       "3   9049.48        1.3      47.13     522.86    2388.08    8133.83     8.3682   \n",
       "4   9055.15        1.3      47.28     522.19    2388.04    8133.80     8.4294   \n",
       "\n",
       "   sensor_16  sensor_17  sensor_18  sensor_19  sensor_20  sensor_21  \n",
       "0       0.03        392       2388      100.0      39.06    23.4190  \n",
       "1       0.03        392       2388      100.0      39.00    23.4236  \n",
       "2       0.03        390       2388      100.0      38.95    23.3442  \n",
       "3       0.03        392       2388      100.0      38.88    23.3739  \n",
       "4       0.03        393       2388      100.0      38.90    23.4044  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset info:\n",
      "Shape: (20631, 26)\n",
      "Units: 100\n",
      "Max cycles per unit: 362\n"
     ]
    }
   ],
   "source": [
    "# Load RUL (Remaining Useful Life) target values for test datasets\n",
    "rul_files = {\n",
    "    'RUL_FD001': 'RUL_FD001.txt',\n",
    "    'RUL_FD002': 'RUL_FD002.txt',\n",
    "    'RUL_FD003': 'RUL_FD003.txt',\n",
    "    'RUL_FD004': 'RUL_FD004.txt'\n",
    "}\n",
    "\n",
    "rul_data = {}\n",
    "print(\"\\nLoading RUL target values...\")\n",
    "for rul_name, filename in rul_files.items():\n",
    "    file_path = DATA_DIR / filename\n",
    "    if file_path.exists():\n",
    "        # RUL files contain single column of target values\n",
    "        rul_values = pd.read_csv(file_path, header=None, names=['rul_target'])\n",
    "        rul_data[rul_name] = rul_values\n",
    "        print(f\"{rul_name}: {len(rul_values)} target values\")\n",
    "    else:\n",
    "        print(f\"{rul_name}: File not found\")\n",
    "\n",
    "# Display sample of first training dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Sample of train_FD001 dataset:\")\n",
    "print(\"=\"*50)\n",
    "if 'train_FD001' in datasets:\n",
    "    display(datasets['train_FD001'].head())\n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"Shape: {datasets['train_FD001'].shape}\")\n",
    "    print(f\"Units: {datasets['train_FD001']['unit_id'].nunique()}\")\n",
    "    print(f\"Max cycles per unit: {datasets['train_FD001'].groupby('unit_id')['time_cycles'].max().max()}\")\n",
    "else:\n",
    "    print(\"train_FD001 not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14acfc22",
   "metadata": {},
   "source": [
    "## 2.2 Schema Validation and Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e28fdf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset_schema(df: pd.DataFrame, dataset_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validate individual dataset schema before combining into fact table.\n",
    "    Checks: column count, data types, positive IDs, missing values.\n",
    "    \"\"\"\n",
    "    validation_results = []\n",
    "    \n",
    "    # Check number of columns\n",
    "    expected_cols = 26\n",
    "    if len(df.columns) != expected_cols:\n",
    "        validation_results.append(f\" Expected {expected_cols} columns, got {len(df.columns)}\")\n",
    "    else:\n",
    "        validation_results.append(f\" Column count: {len(df.columns)}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    if missing_count > 0:\n",
    "        validation_results.append(f\" Missing values: {missing_count}\")\n",
    "    else:\n",
    "        validation_results.append(f\" No missing values\")\n",
    "    \n",
    "    # Check data types (should be numeric)\n",
    "    non_numeric_cols = []\n",
    "    for col in df.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            non_numeric_cols.append(col)\n",
    "    \n",
    "    if non_numeric_cols:\n",
    "        validation_results.append(f\" Non-numeric columns: {non_numeric_cols}\")\n",
    "    else:\n",
    "        validation_results.append(f\" All columns are numeric\")\n",
    "    \n",
    "    # Check unit_id and time_cycles are positive integers\n",
    "    if 'unit_id' in df.columns:\n",
    "        if not (df['unit_id'] > 0).all():\n",
    "            validation_results.append(f\" unit_id contains non-positive values\")\n",
    "        else:\n",
    "            validation_results.append(f\" unit_id values are positive\")\n",
    "    \n",
    "    if 'time_cycles' in df.columns:\n",
    "        if not (df['time_cycles'] > 0).all():\n",
    "            validation_results.append(f\" time_cycles contains non-positive values\")\n",
    "        else:\n",
    "            validation_results.append(f\" time_cycles values are positive\")\n",
    "    \n",
    "    # Return True if all checks passed\n",
    "    all_passed = all('ok' in result for result in validation_results)\n",
    "    return all_passed, validation_results\n",
    "\n",
    "\n",
    "def perform_data_quality_checks(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Perform data quality checks on fact table.\n",
    "    Checks: missing values, duplicates, data type consistency.\n",
    "    \"\"\"\n",
    "    quality_report = {}\n",
    "    \n",
    "    # 1. Missing Values Check\n",
    "    missing_values = df.isnull().sum()\n",
    "    quality_report['missing_values'] = missing_values[missing_values > 0].to_dict()\n",
    "    \n",
    "    # 2. Duplicate Records Check\n",
    "    duplicates = df.duplicated().sum()\n",
    "    quality_report['duplicate_records'] = duplicates\n",
    "    \n",
    "    # 3. Data Type Consistency\n",
    "    numeric_cols = [col for col in df.columns if col.startswith(('sensor_', 'op_setting_'))]\n",
    "    quality_report['non_numeric_in_numeric_cols'] = {}\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            non_numeric = pd.to_numeric(df[col], errors='coerce').isnull().sum()\n",
    "            if non_numeric > 0:\n",
    "                quality_report['non_numeric_in_numeric_cols'][col] = non_numeric\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: SCHEMA VALIDATION (Individual Datasets)\n",
    "# =============================================================================\n",
    "\n",
    "validation_summary = {}\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    if not df.empty:\n",
    "        is_valid, validation_results = validate_dataset_schema(df, dataset_name)\n",
    "        validation_summary[dataset_name] = is_valid\n",
    "        \n",
    "        # Convert data types for better performance\n",
    "        if is_valid:\n",
    "            # Convert unit_id and time_cycles to integers\n",
    "            df['unit_id'] = df['unit_id'].astype('int32')\n",
    "            df['time_cycles'] = df['time_cycles'].astype('int32')\n",
    "            \n",
    "            # Convert sensor readings to float32 for memory efficiency\n",
    "            sensor_cols = [col for col in df.columns if col.startswith('sensor_') or col.startswith('op_setting_')]\n",
    "            df[sensor_cols] = df[sensor_cols].astype('float32')\n",
    "\n",
    "# Wait to display results until after fact table creation\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: DATA QUALITY CHECKS (Fact Table)\n",
    "# =============================================================================\n",
    "# This will be run after fact table is created in section 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239344d",
   "metadata": {},
   "source": [
    "## 2.3 Metadata Mapping and Fact Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0329d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FACT TABLE COLUMN ORGANIZATION\n",
      "================================================================================\n",
      "\n",
      "Total columns: 32\n",
      "\n",
      "1. NEW METADATA COLUMNS (6 created):\n",
      "   1. global_unit_id\n",
      "   2. dataset_id\n",
      "   3. dataset_type\n",
      "   4. conditions_count\n",
      "   5. fault_modes_count\n",
      "   6. dataset_description\n",
      "\n",
      "2. ORIGINAL ID COLUMNS (2 preserved):\n",
      "   1. unit_id\n",
      "   2. time_cycles\n",
      "\n",
      "3. ORIGINAL FEATURE COLUMNS (24 preserved):\n",
      "   - Operational settings: ['op_setting_1', 'op_setting_2', 'op_setting_3']\n",
      "   - Sensors: sensor_1 through sensor_21 (21 sensors)\n",
      "\n",
      "Column order in fact table:\n",
      "   6 new metadata → 2 original IDs → 24 original features = 32 columns\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_unit_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>conditions_count</th>\n",
       "      <th>fault_modes_count</th>\n",
       "      <th>dataset_description</th>\n",
       "      <th>unit_id</th>\n",
       "      <th>time_cycles</th>\n",
       "      <th>op_setting_1</th>\n",
       "      <th>op_setting_2</th>\n",
       "      <th>op_setting_3</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "      <th>sensor_13</th>\n",
       "      <th>sensor_14</th>\n",
       "      <th>sensor_15</th>\n",
       "      <th>sensor_16</th>\n",
       "      <th>sensor_17</th>\n",
       "      <th>sensor_18</th>\n",
       "      <th>sensor_19</th>\n",
       "      <th>sensor_20</th>\n",
       "      <th>sensor_21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FD001_1</td>\n",
       "      <td>FD001</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sea Level, HPC Degradation</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.36</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9046.19</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.47</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FD001_1</td>\n",
       "      <td>FD001</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sea Level, HPC Degradation</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>553.75</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>9044.07</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.49</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FD001_1</td>\n",
       "      <td>FD001</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sea Level, HPC Degradation</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.26</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>9052.94</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.27</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FD001_1</td>\n",
       "      <td>FD001</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sea Level, HPC Degradation</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.45</td>\n",
       "      <td>2388.11</td>\n",
       "      <td>9049.48</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.13</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FD001_1</td>\n",
       "      <td>FD001</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sea Level, HPC Degradation</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.00</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9055.15</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.28</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FD001_1</td>\n",
       "      <td>FD001</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sea Level, HPC Degradation</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.10</td>\n",
       "      <td>1584.47</td>\n",
       "      <td>1398.37</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.67</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>9049.68</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.16</td>\n",
       "      <td>521.68</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8132.85</td>\n",
       "      <td>8.4108</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.98</td>\n",
       "      <td>23.3669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FD001_1</td>\n",
       "      <td>FD001</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sea Level, HPC Degradation</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.48</td>\n",
       "      <td>1592.32</td>\n",
       "      <td>1397.77</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.34</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>9059.13</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.36</td>\n",
       "      <td>522.32</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8132.32</td>\n",
       "      <td>8.3974</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.10</td>\n",
       "      <td>23.3774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FD001_1</td>\n",
       "      <td>FD001</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sea Level, HPC Degradation</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.56</td>\n",
       "      <td>1582.96</td>\n",
       "      <td>1400.97</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>553.85</td>\n",
       "      <td>2388.00</td>\n",
       "      <td>9040.80</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.24</td>\n",
       "      <td>522.47</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8131.07</td>\n",
       "      <td>8.4076</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.97</td>\n",
       "      <td>23.3106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FD001_1</td>\n",
       "      <td>FD001</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sea Level, HPC Degradation</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.12</td>\n",
       "      <td>1590.98</td>\n",
       "      <td>1394.80</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>553.69</td>\n",
       "      <td>2388.05</td>\n",
       "      <td>9046.46</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.29</td>\n",
       "      <td>521.79</td>\n",
       "      <td>2388.05</td>\n",
       "      <td>8125.69</td>\n",
       "      <td>8.3728</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.05</td>\n",
       "      <td>23.4066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FD001_1</td>\n",
       "      <td>FD001</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sea Level, HPC Degradation</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.0033</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.71</td>\n",
       "      <td>1591.24</td>\n",
       "      <td>1400.46</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>553.59</td>\n",
       "      <td>2388.05</td>\n",
       "      <td>9051.70</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.03</td>\n",
       "      <td>521.79</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8129.38</td>\n",
       "      <td>8.4286</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.4694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  global_unit_id dataset_id dataset_type  conditions_count  fault_modes_count  \\\n",
       "0        FD001_1      FD001        train                 1                  1   \n",
       "1        FD001_1      FD001        train                 1                  1   \n",
       "2        FD001_1      FD001        train                 1                  1   \n",
       "3        FD001_1      FD001        train                 1                  1   \n",
       "4        FD001_1      FD001        train                 1                  1   \n",
       "5        FD001_1      FD001        train                 1                  1   \n",
       "6        FD001_1      FD001        train                 1                  1   \n",
       "7        FD001_1      FD001        train                 1                  1   \n",
       "8        FD001_1      FD001        train                 1                  1   \n",
       "9        FD001_1      FD001        train                 1                  1   \n",
       "\n",
       "          dataset_description  unit_id  time_cycles  op_setting_1  \\\n",
       "0  Sea Level, HPC Degradation        1            1       -0.0007   \n",
       "1  Sea Level, HPC Degradation        1            2        0.0019   \n",
       "2  Sea Level, HPC Degradation        1            3       -0.0043   \n",
       "3  Sea Level, HPC Degradation        1            4        0.0007   \n",
       "4  Sea Level, HPC Degradation        1            5       -0.0019   \n",
       "5  Sea Level, HPC Degradation        1            6       -0.0043   \n",
       "6  Sea Level, HPC Degradation        1            7        0.0010   \n",
       "7  Sea Level, HPC Degradation        1            8       -0.0034   \n",
       "8  Sea Level, HPC Degradation        1            9        0.0008   \n",
       "9  Sea Level, HPC Degradation        1           10       -0.0033   \n",
       "\n",
       "   op_setting_2  op_setting_3  sensor_1  sensor_2  sensor_3  sensor_4  \\\n",
       "0       -0.0004         100.0    518.67    641.82   1589.70   1400.60   \n",
       "1       -0.0003         100.0    518.67    642.15   1591.82   1403.14   \n",
       "2        0.0003         100.0    518.67    642.35   1587.99   1404.20   \n",
       "3        0.0000         100.0    518.67    642.35   1582.79   1401.87   \n",
       "4       -0.0002         100.0    518.67    642.37   1582.85   1406.22   \n",
       "5       -0.0001         100.0    518.67    642.10   1584.47   1398.37   \n",
       "6        0.0001         100.0    518.67    642.48   1592.32   1397.77   \n",
       "7        0.0003         100.0    518.67    642.56   1582.96   1400.97   \n",
       "8        0.0001         100.0    518.67    642.12   1590.98   1394.80   \n",
       "9        0.0001         100.0    518.67    641.71   1591.24   1400.46   \n",
       "\n",
       "   sensor_5  sensor_6  sensor_7  sensor_8  sensor_9  sensor_10  sensor_11  \\\n",
       "0     14.62     21.61    554.36   2388.06   9046.19        1.3      47.47   \n",
       "1     14.62     21.61    553.75   2388.04   9044.07        1.3      47.49   \n",
       "2     14.62     21.61    554.26   2388.08   9052.94        1.3      47.27   \n",
       "3     14.62     21.61    554.45   2388.11   9049.48        1.3      47.13   \n",
       "4     14.62     21.61    554.00   2388.06   9055.15        1.3      47.28   \n",
       "5     14.62     21.61    554.67   2388.02   9049.68        1.3      47.16   \n",
       "6     14.62     21.61    554.34   2388.02   9059.13        1.3      47.36   \n",
       "7     14.62     21.61    553.85   2388.00   9040.80        1.3      47.24   \n",
       "8     14.62     21.61    553.69   2388.05   9046.46        1.3      47.29   \n",
       "9     14.62     21.61    553.59   2388.05   9051.70        1.3      47.03   \n",
       "\n",
       "   sensor_12  sensor_13  sensor_14  sensor_15  sensor_16  sensor_17  \\\n",
       "0     521.66    2388.02    8138.62     8.4195       0.03        392   \n",
       "1     522.28    2388.07    8131.49     8.4318       0.03        392   \n",
       "2     522.42    2388.03    8133.23     8.4178       0.03        390   \n",
       "3     522.86    2388.08    8133.83     8.3682       0.03        392   \n",
       "4     522.19    2388.04    8133.80     8.4294       0.03        393   \n",
       "5     521.68    2388.03    8132.85     8.4108       0.03        391   \n",
       "6     522.32    2388.03    8132.32     8.3974       0.03        392   \n",
       "7     522.47    2388.03    8131.07     8.4076       0.03        391   \n",
       "8     521.79    2388.05    8125.69     8.3728       0.03        392   \n",
       "9     521.79    2388.06    8129.38     8.4286       0.03        393   \n",
       "\n",
       "   sensor_18  sensor_19  sensor_20  sensor_21  \n",
       "0       2388      100.0      39.06    23.4190  \n",
       "1       2388      100.0      39.00    23.4236  \n",
       "2       2388      100.0      38.95    23.3442  \n",
       "3       2388      100.0      38.88    23.3739  \n",
       "4       2388      100.0      38.90    23.4044  \n",
       "5       2388      100.0      38.98    23.3669  \n",
       "6       2388      100.0      39.10    23.3774  \n",
       "7       2388      100.0      38.97    23.3106  \n",
       "8       2388      100.0      39.05    23.4066  \n",
       "9       2388      100.0      38.95    23.4694  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 265256 entries, 0 to 265255\n",
      "Data columns (total 32 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   global_unit_id       265256 non-null  object \n",
      " 1   dataset_id           265256 non-null  object \n",
      " 2   dataset_type         265256 non-null  object \n",
      " 3   conditions_count     265256 non-null  int64  \n",
      " 4   fault_modes_count    265256 non-null  int64  \n",
      " 5   dataset_description  265256 non-null  object \n",
      " 6   unit_id              265256 non-null  int64  \n",
      " 7   time_cycles          265256 non-null  int64  \n",
      " 8   op_setting_1         265256 non-null  float64\n",
      " 9   op_setting_2         265256 non-null  float64\n",
      " 10  op_setting_3         265256 non-null  float64\n",
      " 11  sensor_1             265256 non-null  float64\n",
      " 12  sensor_2             265256 non-null  float64\n",
      " 13  sensor_3             265256 non-null  float64\n",
      " 14  sensor_4             265256 non-null  float64\n",
      " 15  sensor_5             265256 non-null  float64\n",
      " 16  sensor_6             265256 non-null  float64\n",
      " 17  sensor_7             265256 non-null  float64\n",
      " 18  sensor_8             265256 non-null  float64\n",
      " 19  sensor_9             265256 non-null  float64\n",
      " 20  sensor_10            265256 non-null  float64\n",
      " 21  sensor_11            265256 non-null  float64\n",
      " 22  sensor_12            265256 non-null  float64\n",
      " 23  sensor_13            265256 non-null  float64\n",
      " 24  sensor_14            265256 non-null  float64\n",
      " 25  sensor_15            265256 non-null  float64\n",
      " 26  sensor_16            265256 non-null  float64\n",
      " 27  sensor_17            265256 non-null  int64  \n",
      " 28  sensor_18            265256 non-null  int64  \n",
      " 29  sensor_19            265256 non-null  float64\n",
      " 30  sensor_20            265256 non-null  float64\n",
      " 31  sensor_21            265256 non-null  float64\n",
      "dtypes: float64(22), int64(6), object(4)\n",
      "memory usage: 64.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'missing_values': {}, 'duplicate_records': np.int64(0), 'non_numeric_in_numeric_cols': {}}\n"
     ]
    }
   ],
   "source": [
    "def create_fact_table(train_datasets: Dict[str, pd.DataFrame], \n",
    "                     test_datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create unified fact table from training and test datasets.\n",
    "    \n",
    "    Adds 6 new metadata columns to track provenance and experimental conditions:\n",
    "    - global_unit_id: Unique identifier across all datasets\n",
    "    - dataset_id: Sub-dataset identifier (FD001-FD004)\n",
    "    - dataset_type: Training or test designation\n",
    "    - conditions_count: Number of operating conditions (1 or 6)\n",
    "    - fault_modes_count: Number of fault modes (1 or 2)\n",
    "    - dataset_description: Human-readable\n",
    "    Preserves all original columns: unit_id, time_cycles, op_settings, sensors\n",
    "    \n",
    "    \"\"\"\n",
    "    fact_table_parts = []\n",
    "    \n",
    "    # Dataset metadata mapping\n",
    "    dataset_metadata = {\n",
    "        'FD001': {'conditions': 1, 'fault_modes': 1, 'description': 'Sea Level, HPC Degradation'},\n",
    "        'FD002': {'conditions': 6, 'fault_modes': 1, 'description': '6 Conditions, HPC Degradation'},\n",
    "        'FD003': {'conditions': 1, 'fault_modes': 2, 'description': 'Sea Level, HPC + Fan Degradation'},\n",
    "        'FD004': {'conditions': 6, 'fault_modes': 2, 'description': '6 Conditions, HPC + Fan Degradation'}\n",
    "    }\n",
    "    \n",
    "    # Process training datasets\n",
    "    for dataset_name, df in train_datasets.items():\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Extract dataset identifier (e.g., 'FD001' from 'train_FD001')\n",
    "        dataset_id = dataset_name.split('_')[1]\n",
    "        \n",
    "        # Add 5 new metadata columns\n",
    "        df_copy['dataset_id'] = dataset_id\n",
    "        df_copy['dataset_type'] = 'train'\n",
    "        df_copy['conditions_count'] = dataset_metadata[dataset_id]['conditions']\n",
    "        df_copy['fault_modes_count'] = dataset_metadata[dataset_id]['fault_modes']\n",
    "        df_copy['dataset_description'] = dataset_metadata[dataset_id]['description']\n",
    "        \n",
    "        # Create unique global unit ID (6th metadata column)\n",
    "        df_copy['global_unit_id'] = dataset_id + '_' + df_copy['unit_id'].astype(str)\n",
    "        \n",
    "        fact_table_parts.append(df_copy)\n",
    "    \n",
    "    # Process test datasets\n",
    "    for dataset_name, df in test_datasets.items():\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Extract dataset identifier\n",
    "        dataset_id = dataset_name.split('_')[1]\n",
    "        \n",
    "        # Add 5 new metadata columns\n",
    "        df_copy['dataset_id'] = dataset_id\n",
    "        df_copy['dataset_type'] = 'test'\n",
    "        df_copy['conditions_count'] = dataset_metadata[dataset_id]['conditions']\n",
    "        df_copy['fault_modes_count'] = dataset_metadata[dataset_id]['fault_modes']\n",
    "        df_copy['dataset_description'] = dataset_metadata[dataset_id]['description']\n",
    "        \n",
    "        # Create unique global unit ID (6th metadata column)\n",
    "        df_copy['global_unit_id'] = dataset_id + '_test_' + df_copy['unit_id'].astype(str)\n",
    "        \n",
    "        fact_table_parts.append(df_copy)\n",
    "    \n",
    "    # Combine all datasets\n",
    "    fact_table = pd.concat(fact_table_parts, ignore_index=True)\n",
    "    \n",
    "    # Reorder columns for better organization\n",
    "    # Place 6 new metadata columns first, then original columns (unit_id, time_cycles, features)\n",
    "    # Note: RUL column will be added later in Section 5.1\n",
    "    new_metadata_cols = ['global_unit_id', 'dataset_id', 'dataset_type', \n",
    "                         'conditions_count', 'fault_modes_count', 'dataset_description']\n",
    "    original_id_cols = ['unit_id', 'time_cycles']\n",
    "    feature_cols = ['op_setting_1', 'op_setting_2', 'op_setting_3'] + \\\n",
    "                  [f'sensor_{i}' for i in range(1, 22)]\n",
    "    \n",
    "    # Combine: new metadata → original IDs → original features\n",
    "    column_order = [col for col in new_metadata_cols + original_id_cols + feature_cols \n",
    "                    if col in fact_table.columns]\n",
    "    fact_table = fact_table[column_order]\n",
    "    \n",
    "    return fact_table\n",
    "\n",
    "\n",
    "# Group datasets by train/test\n",
    "train_dict = {name: df for name, df in datasets.items() if 'train' in name.lower()}\n",
    "test_dict = {name: df for name, df in datasets.items() if 'test' in name.lower()}\n",
    "\n",
    "# Create fact table\n",
    "fact_table = create_fact_table(train_dict, test_dict)\n",
    "\n",
    "# Display column organization\n",
    "print(\"=\" * 80)\n",
    "print(\"FACT TABLE COLUMN ORGANIZATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal columns: {len(fact_table.columns)}\")\n",
    "print(f\"\\n1. NEW METADATA COLUMNS (6 created):\")\n",
    "new_metadata_cols = ['global_unit_id', 'dataset_id', 'dataset_type', \n",
    "                     'conditions_count', 'fault_modes_count', 'dataset_description']\n",
    "for i, col in enumerate(new_metadata_cols, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "\n",
    "print(f\"\\n2. ORIGINAL ID COLUMNS (2 preserved):\")\n",
    "original_id_cols = ['unit_id', 'time_cycles']\n",
    "for i, col in enumerate(original_id_cols, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "\n",
    "print(f\"\\n3. ORIGINAL FEATURE COLUMNS (24 preserved):\")\n",
    "feature_cols = ['op_setting_1', 'op_setting_2', 'op_setting_3'] + \\\n",
    "              [f'sensor_{i}' for i in range(1, 22)]\n",
    "print(f\"   - Operational settings: {['op_setting_1', 'op_setting_2', 'op_setting_3']}\")\n",
    "print(f\"   - Sensors: sensor_1 through sensor_21 (21 sensors)\")\n",
    "\n",
    "print(f\"\\nColumn order in fact table:\")\n",
    "print(f\"   6 new metadata → 2 original IDs → 24 original features = {6 + 2 + 24} columns\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# DISPLAY VALIDATION RESULTS (From Section 2.2)\n",
    "# =============================================================================\n",
    "\n",
    "# Display sample of the fact table\n",
    "display(fact_table.head(10))\n",
    "display(fact_table.info())\n",
    "\n",
    "# =============================================================================\n",
    "# RUN DATA QUALITY CHECKS ON FACT TABLE\n",
    "# =============================================================================\n",
    "\n",
    "quality_report = perform_data_quality_checks(fact_table)\n",
    "\n",
    "print(quality_report)\n",
    "# Display validation results for individual datasets\n",
    "\n",
    "all_valid = all(validation_summary.values())\n",
    "\n",
    "\n",
    "# Display quality report for fact table\n",
    "\n",
    "\n",
    "# Additional basic statistics\n",
    "sensor_cols = [col for col in fact_table.columns if col.startswith('sensor_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8957fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdjhavera\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/davidhavera/CS598_FDC/wandb/run-20251027_112420-4y2iwmhp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/djhavera/cmapss-data-curation/runs/4y2iwmhp?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">data-loading-20251027-112420</a></strong> to <a href='https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/djhavera/cmapss-data-curation/runs/4y2iwmhp?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">https://wandb.ai/djhavera/cmapss-data-curation/runs/4y2iwmhp?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating W&B artifact for raw fact_table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Artifacts logged anonymously cannot be claimed and expire after 7 days.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artifact logged: fact_table_raw:v0\n",
      "   Rows: 265,256\n",
      "   Columns: 32\n",
      "🔗 Run URL: https://wandb.ai/djhavera/cmapss-data-curation/runs/4y2iwmhp?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">data-loading-20251027-112420</strong> at: <a href='https://wandb.ai/djhavera/cmapss-data-curation/runs/4y2iwmhp?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">https://wandb.ai/djhavera/cmapss-data-curation/runs/4y2iwmhp?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72</a><br> View project at: <a href='https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251027_112420-4y2iwmhp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# M12: Log raw fact_table as W&B artifact (Step 1 in lineage)\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "\n",
    "# Initialize W&B for data loading step\n",
    "data_loading_run = wandb.init(\n",
    "    project=\"cmapss-data-curation\",\n",
    "    name=f\"data-loading-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    job_type=\"data-loading\",\n",
    "    config={\n",
    "        \"num_datasets\": 4,\n",
    "        \"datasets\": [\"FD001\", \"FD002\", \"FD003\", \"FD004\"],\n",
    "        \"train_test_split\": \"original\",\n",
    "        \"module\": \"M7 - What is Data?\"\n",
    "    },\n",
    "    tags=[\"data-loading\", \"raw-data\", \"m7\"],\n",
    "    anonymous=\"allow\"\n",
    ")\n",
    "\n",
    "print(\" Creating W&B artifact for raw fact_table...\")\n",
    "\n",
    "# Save raw fact_table temporarily\n",
    "fact_table.to_csv(\"fact_table_raw.csv\", index=False)\n",
    "\n",
    "# Create artifact\n",
    "raw_artifact = wandb.Artifact(\n",
    "    name=\"fact_table_raw\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Raw CMAPSS fact table with all 4 datasets (FD001-FD004) before outlier removal\",\n",
    "    metadata={\n",
    "        \"rows\": len(fact_table),\n",
    "        \"columns\": len(fact_table.columns),\n",
    "        \"datasets\": [\"FD001\", \"FD002\", \"FD003\", \"FD004\"],\n",
    "        \"train_units\": fact_table[fact_table['dataset_type']=='train']['unit_id'].nunique(),\n",
    "        \"test_units\": fact_table[fact_table['dataset_type']=='test']['unit_id'].nunique(),\n",
    "        \"module\": \"M7 - What is Data?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_artifact.add_file(\"fact_table_raw.csv\")\n",
    "data_loading_run.log_artifact(raw_artifact)\n",
    "\n",
    "print(f\" Artifact logged: fact_table_raw:v0\")\n",
    "print(f\"   Rows: {len(fact_table):,}\")\n",
    "print(f\"   Columns: {len(fact_table.columns)}\")\n",
    "print(f\"🔗 Run URL: {data_loading_run.get_url()}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97614e8b",
   "metadata": {},
   "source": [
    "## 3.1 Outlier Detection and Removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770f76f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. STATISTICS-BASED OUTLIER DETECTION\n",
      "----------------------------------------------------------------------\n",
      "Analyzing 21 numeric columns...\n",
      "\n",
      "Computing univariate outliers across all records...\n",
      "\n",
      "Example results for first 5 numeric columns:\n",
      "sensor_1:\n",
      "  Hampel X84:       0 outliers ( 0.00%)\n",
      "  Z-score:          0 outliers ( 0.00%)\n",
      "  Median: 489.05, MAD: 29.62 | Mean: 485.82, Std: 30.42\n",
      "\n",
      "sensor_2:\n",
      "  Hampel X84:       0 outliers ( 0.00%)\n",
      "  Z-score:          0 outliers ( 0.00%)\n",
      "  Median: 605.81, MAD: 36.96 | Mean: 597.28, Std: 42.46\n",
      "\n",
      "sensor_3:\n",
      "  Hampel X84:       0 outliers ( 0.00%)\n",
      "  Z-score:          0 outliers ( 0.00%)\n",
      "  Median: 1491.81, MAD: 100.00 | Mean: 1466.13, Std: 118.05\n",
      "\n",
      "sensor_4:\n",
      "  Hampel X84:       0 outliers ( 0.00%)\n",
      "  Z-score:          0 outliers ( 0.00%)\n",
      "  Median: 1269.20, MAD: 136.66 | Mean: 1259.49, Std: 136.09\n",
      "\n",
      "sensor_5:\n",
      "  Hampel X84:       0 outliers ( 0.00%)\n",
      "  Z-score:          0 outliers ( 0.00%)\n",
      "  Median: 9.35, MAD: 5.27 | Mean: 9.89, Std: 4.27\n",
      "\n",
      "\n",
      "Aggregate Statistics-Based Results:\n",
      "  Hampel X84 flagged: 133,586 records (50.36%)\n",
      "  Z-score flagged:    1 records (0.00%)\n",
      "  (Record flagged if ANY column is outlier)\n",
      "\n",
      "✓ Statistics-based detection complete\n",
      "\n",
      "\n",
      "2. DISTANCE-BASED OUTLIER DETECTION\n",
      "----------------------------------------------------------------------\n",
      "Applying Local Outlier Factor (LOF) to 9 sensors\n",
      "Sample size: 10,000 records\n",
      "\n",
      "\n",
      "Example results for first 5 numeric columns:\n",
      "sensor_1:\n",
      "  Hampel X84:       0 outliers ( 0.00%)\n",
      "  Z-score:          0 outliers ( 0.00%)\n",
      "  Median: 489.05, MAD: 29.62 | Mean: 485.82, Std: 30.42\n",
      "\n",
      "sensor_2:\n",
      "  Hampel X84:       0 outliers ( 0.00%)\n",
      "  Z-score:          0 outliers ( 0.00%)\n",
      "  Median: 605.81, MAD: 36.96 | Mean: 597.28, Std: 42.46\n",
      "\n",
      "sensor_3:\n",
      "  Hampel X84:       0 outliers ( 0.00%)\n",
      "  Z-score:          0 outliers ( 0.00%)\n",
      "  Median: 1491.81, MAD: 100.00 | Mean: 1466.13, Std: 118.05\n",
      "\n",
      "sensor_4:\n",
      "  Hampel X84:       0 outliers ( 0.00%)\n",
      "  Z-score:          0 outliers ( 0.00%)\n",
      "  Median: 1269.20, MAD: 136.66 | Mean: 1259.49, Std: 136.09\n",
      "\n",
      "sensor_5:\n",
      "  Hampel X84:       0 outliers ( 0.00%)\n",
      "  Z-score:          0 outliers ( 0.00%)\n",
      "  Median: 9.35, MAD: 5.27 | Mean: 9.89, Std: 4.27\n",
      "\n",
      "\n",
      "Aggregate Statistics-Based Results:\n",
      "  Hampel X84 flagged: 133,586 records (50.36%)\n",
      "  Z-score flagged:    1 records (0.00%)\n",
      "  (Record flagged if ANY column is outlier)\n",
      "\n",
      "✓ Statistics-based detection complete\n",
      "\n",
      "\n",
      "2. DISTANCE-BASED OUTLIER DETECTION\n",
      "----------------------------------------------------------------------\n",
      "Applying Local Outlier Factor (LOF) to 9 sensors\n",
      "Sample size: 10,000 records\n",
      "\n",
      "LOF Results:\n",
      "  Outliers detected: 500 (5.00%)\n",
      "  LOF score range: [-3.098, -0.955]\n",
      "  (Lower scores indicate stronger outliers)\n",
      "\n",
      "Applying Isolation Forest\n",
      "LOF Results:\n",
      "  Outliers detected: 500 (5.00%)\n",
      "  LOF score range: [-3.098, -0.955]\n",
      "  (Lower scores indicate stronger outliers)\n",
      "\n",
      "Applying Isolation Forest\n",
      "Isolation Forest Results:\n",
      "  Outliers detected: 500 (5.00%)\n",
      "  Anomaly score range: [-0.697, -0.394]\n",
      "\n",
      " Distance-based detection complete\n",
      "\n",
      "\n",
      "3. MULTIVARIATE OUTLIER DETECTION\n",
      "----------------------------------------------------------------------\n",
      "Using Robust Covariance (MCD) with Mahalanobis Distance\n",
      "Sample size: 10,000 records\n",
      "\n",
      "Isolation Forest Results:\n",
      "  Outliers detected: 500 (5.00%)\n",
      "  Anomaly score range: [-0.697, -0.394]\n",
      "\n",
      " Distance-based detection complete\n",
      "\n",
      "\n",
      "3. MULTIVARIATE OUTLIER DETECTION\n",
      "----------------------------------------------------------------------\n",
      "Using Robust Covariance (MCD) with Mahalanobis Distance\n",
      "Sample size: 10,000 records\n",
      "\n",
      "Mahalanobis Distance Results:\n",
      "  Outliers detected: 4,774 (47.74%)\n",
      "  Threshold (χ² 95%): 16.92\n",
      "  Max distance: 11078.84\n",
      "  Mean distance: 1281.30\n",
      "\n",
      "✓ Multivariate detection complete\n",
      "\n",
      "\n",
      "4. CONSENSUS OUTLIER DETECTION\n",
      "----------------------------------------------------------------------\n",
      "Combining all 5 detection methods for robust consensus\n",
      "\n",
      "Consensus Results (3+ methods agree):\n",
      "  Outliers by 5 methods: 0\n",
      "  Outliers by 4 methods: 100\n",
      "  Outliers by 3 methods: 442\n",
      "  Outliers by 2 methods: 3,203\n",
      "  Outliers by 1 method:  2,754\n",
      "  Total consensus outliers: 542 (5.42%)\n",
      "\n",
      "Method Contribution to Consensus Outliers:\n",
      "  Hampel X84:           525 ( 96.9%)\n",
      "  Z-score:                0 (  0.0%)\n",
      "  LOF:                  314 ( 57.9%)\n",
      "  Isolation Forest:     345 ( 63.7%)\n",
      "  Mahalanobis:          542 (100.0%)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "OUTLIER DETECTION SUMMARY\n",
      "======================================================================\n",
      "Dataset size: 265,256 records\n",
      "Analysis sample: 10,000 records\n",
      "\n",
      "======================================================================\n",
      "Mahalanobis Distance Results:\n",
      "  Outliers detected: 4,774 (47.74%)\n",
      "  Threshold (χ² 95%): 16.92\n",
      "  Max distance: 11078.84\n",
      "  Mean distance: 1281.30\n",
      "\n",
      "✓ Multivariate detection complete\n",
      "\n",
      "\n",
      "4. CONSENSUS OUTLIER DETECTION\n",
      "----------------------------------------------------------------------\n",
      "Combining all 5 detection methods for robust consensus\n",
      "\n",
      "Consensus Results (3+ methods agree):\n",
      "  Outliers by 5 methods: 0\n",
      "  Outliers by 4 methods: 100\n",
      "  Outliers by 3 methods: 442\n",
      "  Outliers by 2 methods: 3,203\n",
      "  Outliers by 1 method:  2,754\n",
      "  Total consensus outliers: 542 (5.42%)\n",
      "\n",
      "Method Contribution to Consensus Outliers:\n",
      "  Hampel X84:           525 ( 96.9%)\n",
      "  Z-score:                0 (  0.0%)\n",
      "  LOF:                  314 ( 57.9%)\n",
      "  Isolation Forest:     345 ( 63.7%)\n",
      "  Mahalanobis:          542 (100.0%)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "OUTLIER DETECTION SUMMARY\n",
      "======================================================================\n",
      "Dataset size: 265,256 records\n",
      "Analysis sample: 10,000 records\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import MinCovDet\n",
    "from scipy.stats import chi2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1. STATISTICS-BASED DETECTION (Hampel X84 & Z-score)\n",
    "# -------------------------\n",
    "print(\"1. STATISTICS-BASED OUTLIER DETECTION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def hampel_outlier_detection(data, theta=3):\n",
    "    median = np.median(data)\n",
    "    mad = np.median(np.abs(data - median))\n",
    "    \n",
    "    # Constant 1.4826 derived under normal distribution\n",
    "    # where 1 std dev ≈ 1.4826 MADs\n",
    "    threshold = 1.4826 * theta * mad\n",
    "    \n",
    "    lower_bound = median - threshold\n",
    "    upper_bound = median + threshold\n",
    "    \n",
    "    outliers = (data < lower_bound) | (data > upper_bound)\n",
    "    return outliers\n",
    "\n",
    "def z_score_outlier_detection(data, threshold=3):\n",
    "\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    z_scores = np.abs((data - mean) / std)\n",
    "    return z_scores > threshold\n",
    "\n",
    "# Apply robust statistics-based detection to sensor columns\n",
    "sensor_cols = [col for col in fact_table.columns if col.startswith('sensor_')]\n",
    "setting_cols = [col for col in fact_table.columns if col.startswith('setting_')]\n",
    "numeric_cols = sensor_cols + setting_cols\n",
    "\n",
    "outlier_summary_stats = {\n",
    "    'hampel': {},\n",
    "    'zscore': {}\n",
    "}\n",
    "\n",
    "print(f\"Analyzing {len(numeric_cols)} numeric columns...\\n\")\n",
    "\n",
    "# Create full-dataset outlier masks for Hampel and Z-score\n",
    "print(\"Computing univariate outliers across all records...\")\n",
    "outliers_hampel_full = np.zeros(len(fact_table), dtype=bool)\n",
    "outliers_zscore_full = np.zeros(len(fact_table), dtype=bool)\n",
    "\n",
    "for col in numeric_cols:\n",
    "    data = fact_table[col].fillna(fact_table[col].median())  # Handle NaN\n",
    "    \n",
    "    # Hampel X84 (Robust)\n",
    "    hampel_outliers = hampel_outlier_detection(data, theta=3)\n",
    "    outliers_hampel_full |= hampel_outliers  # Union: mark as outlier if ANY column flags it\n",
    "    outlier_summary_stats['hampel'][col] = hampel_outliers.sum()\n",
    "    \n",
    "    # Z-score (Non-robust, for comparison)\n",
    "    zscore_outliers = z_score_outlier_detection(data, threshold=3)\n",
    "    outliers_zscore_full |= zscore_outliers\n",
    "    outlier_summary_stats['zscore'][col] = zscore_outliers.sum()\n",
    "\n",
    "# Show examples for first 5 columns\n",
    "print(\"\\nExample results for first 5 numeric columns:\")\n",
    "for col in numeric_cols[:5]:\n",
    "    data = fact_table[col].dropna()\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Hampel X84:  {outlier_summary_stats['hampel'][col]:>6,} outliers ({outlier_summary_stats['hampel'][col]/len(data)*100:>5.2f}%)\")\n",
    "    print(f\"  Z-score:     {outlier_summary_stats['zscore'][col]:>6,} outliers ({outlier_summary_stats['zscore'][col]/len(data)*100:>5.2f}%)\")\n",
    "    \n",
    "    # Show statistics\n",
    "    median = np.median(data)\n",
    "    mad = np.median(np.abs(data - median))\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    print(f\"  Median: {median:.2f}, MAD: {mad:.2f} | Mean: {mean:.2f}, Std: {std:.2f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nAggregate Statistics-Based Results:\")\n",
    "print(f\"  Hampel X84 flagged: {outliers_hampel_full.sum():,} records ({outliers_hampel_full.sum()/len(fact_table)*100:.2f}%)\")\n",
    "print(f\"  Z-score flagged:    {outliers_zscore_full.sum():,} records ({outliers_zscore_full.sum()/len(fact_table)*100:.2f}%)\")\n",
    "print(f\"  (Record flagged if ANY column is outlier)\")\n",
    "print()\n",
    "print(f\"✓ Statistics-based detection complete\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. DISTANCE-BASED OUTLIER DETECTION (LOF)\n",
    "# -------------------------\n",
    "print(\"\\n2. DISTANCE-BASED OUTLIER DETECTION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Select subset of sensors for LOF (computationally expensive for all)\n",
    "sample_sensors = ['sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_11', \n",
    "                  'sensor_12', 'sensor_15', 'sensor_20', 'sensor_21']\n",
    "sample_sensors = [s for s in sample_sensors if s in fact_table.columns]\n",
    "\n",
    "# Sample data for LOF (LOF is O(n²), use representative sample)\n",
    "sample_size = min(10000, len(fact_table))\n",
    "sample_indices = np.random.choice(fact_table.index, sample_size, replace=False)\n",
    "lof_data = fact_table.loc[sample_indices, sample_sensors].dropna()\n",
    "\n",
    "print(f\"Applying Local Outlier Factor (LOF) to {len(sample_sensors)} sensors\")\n",
    "print(f\"Sample size: {len(lof_data):,} records\")\n",
    "print()\n",
    "\n",
    "# LOF: Detects outliers based on local density deviation\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "lof_predictions = lof.fit_predict(lof_data)\n",
    "lof_scores = lof.negative_outlier_factor_\n",
    "\n",
    "outliers_lof = (lof_predictions == -1)\n",
    "print(f\"LOF Results:\")\n",
    "print(f\"  Outliers detected: {outliers_lof.sum():,} ({outliers_lof.sum()/len(lof_data)*100:.2f}%)\")\n",
    "print(f\"  LOF score range: [{lof_scores.min():.3f}, {lof_scores.max():.3f}]\")\n",
    "print(f\"  (Lower scores indicate stronger outliers)\")\n",
    "print()\n",
    "\n",
    "# Isolation Forest: Uses random partitioning\n",
    "print(\"Applying Isolation Forest\")\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso_predictions = iso_forest.fit_predict(lof_data)\n",
    "iso_scores = iso_forest.score_samples(lof_data)\n",
    "\n",
    "outliers_iso = (iso_predictions == -1)\n",
    "print(f\"Isolation Forest Results:\")\n",
    "print(f\"  Outliers detected: {outliers_iso.sum():,} ({outliers_iso.sum()/len(lof_data)*100:.2f}%)\")\n",
    "print(f\"  Anomaly score range: [{iso_scores.min():.3f}, {iso_scores.max():.3f}]\")\n",
    "print()\n",
    "\n",
    "print(f\" Distance-based detection complete\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. MULTIVARIATE OUTLIER DETECTION (Mahalanobis Distance)\n",
    "# -------------------------\n",
    "print(\"\\n3. MULTIVARIATE OUTLIER DETECTION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Use robust covariance estimation (MCD - Minimum Covariance Determinant)\n",
    "# MCD has 50% breakdown point\n",
    "print(\"Using Robust Covariance (MCD) with Mahalanobis Distance\")\n",
    "print(f\"Sample size: {len(lof_data):,} records\")\n",
    "print()\n",
    "\n",
    "# Fit robust covariance\n",
    "mcd = MinCovDet(random_state=42).fit(lof_data)\n",
    "mahal_distances = mcd.mahalanobis(lof_data)\n",
    "\n",
    "# Threshold using chi-square distribution\n",
    "# Degrees of freedom = number of dimensions\n",
    "df = lof_data.shape[1]\n",
    "threshold = chi2.ppf(0.95, df)  # 95% confidence\n",
    "\n",
    "outliers_mahal = mahal_distances > threshold\n",
    "\n",
    "print(f\"Mahalanobis Distance Results:\")\n",
    "print(f\"  Outliers detected: {outliers_mahal.sum():,} ({outliers_mahal.sum()/len(lof_data)*100:.2f}%)\")\n",
    "print(f\"  Threshold (χ² 95%): {threshold:.2f}\")\n",
    "print(f\"  Max distance: {mahal_distances.max():.2f}\")\n",
    "print(f\"  Mean distance: {mahal_distances.mean():.2f}\")\n",
    "print()\n",
    "\n",
    "print(f\"✓ Multivariate detection complete\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. CONSENSUS OUTLIER DETECTION (All 5 Methods)\n",
    "# -------------------------\n",
    "print(\"\\n4. CONSENSUS OUTLIER DETECTION\")\n",
    "print(\"-\" * 70)\n",
    "print(\"Combining all 5 detection methods for robust consensus\")\n",
    "print()\n",
    "\n",
    "# Map full-dataset statistics methods to sampled indices\n",
    "outliers_hampel_sample = outliers_hampel_full[sample_indices]\n",
    "outliers_zscore_sample = outliers_zscore_full[sample_indices]\n",
    "\n",
    "# Combine all 5 methods: Flag as outlier if detected by multiple methods\n",
    "outlier_votes = np.zeros(len(lof_data), dtype=int)\n",
    "outlier_votes += outliers_hampel_sample.astype(int)  # Statistics: Hampel\n",
    "outlier_votes += outliers_zscore_sample.astype(int)  # Statistics: Z-score\n",
    "outlier_votes += outliers_lof.astype(int)            # Distance: LOF\n",
    "outlier_votes += outliers_iso.astype(int)            # Distance: Isolation Forest\n",
    "outlier_votes += outliers_mahal.astype(int)          # Multivariate: Mahalanobis\n",
    "\n",
    "# Require at least 3 out of 5 methods to agree\n",
    "consensus_outliers = outlier_votes >= 3\n",
    "\n",
    "print(f\"Consensus Results (3+ methods agree):\")\n",
    "print(f\"  Outliers by 5 methods: {(outlier_votes == 5).sum():,}\")\n",
    "print(f\"  Outliers by 4 methods: {(outlier_votes == 4).sum():,}\")\n",
    "print(f\"  Outliers by 3 methods: {(outlier_votes == 3).sum():,}\")\n",
    "print(f\"  Outliers by 2 methods: {(outlier_votes == 2).sum():,}\")\n",
    "print(f\"  Outliers by 1 method:  {(outlier_votes == 1).sum():,}\")\n",
    "print(f\"  Total consensus outliers: {consensus_outliers.sum():,} ({consensus_outliers.sum()/len(lof_data)*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Show method contribution breakdown\n",
    "print(\"Method Contribution to Consensus Outliers:\")\n",
    "if consensus_outliers.sum() > 0:\n",
    "    # Use boolean indexing instead of positional indexing\n",
    "    hampel_contrib = outliers_hampel_sample[consensus_outliers].sum()\n",
    "    zscore_contrib = outliers_zscore_sample[consensus_outliers].sum()\n",
    "    lof_contrib = outliers_lof[consensus_outliers].sum()\n",
    "    iso_contrib = outliers_iso[consensus_outliers].sum()\n",
    "    mahal_contrib = outliers_mahal[consensus_outliers].sum()\n",
    "    \n",
    "    total_consensus = consensus_outliers.sum()\n",
    "    print(f\"  Hampel X84:         {hampel_contrib:>5,} ({hampel_contrib/total_consensus*100:>5.1f}%)\")\n",
    "    print(f\"  Z-score:            {zscore_contrib:>5,} ({zscore_contrib/total_consensus*100:>5.1f}%)\")\n",
    "    print(f\"  LOF:                {lof_contrib:>5,} ({lof_contrib/total_consensus*100:>5.1f}%)\")\n",
    "    print(f\"  Isolation Forest:   {iso_contrib:>5,} ({iso_contrib/total_consensus*100:>5.1f}%)\")\n",
    "    print(f\"  Mahalanobis:        {mahal_contrib:>5,} ({mahal_contrib/total_consensus*100:>5.1f}%)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# -------------------------\n",
    "# SUMMARY\n",
    "# -------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OUTLIER DETECTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dataset size: {len(fact_table):,} records\")\n",
    "print(f\"Analysis sample: {len(lof_data):,} records\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bca2cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONSENSUS-BASED OUTLIER REMOVAL\n",
      "======================================================================\n",
      "Removing outliers identified by 3+ detection methods (out of 5)\n",
      "\n",
      "Original fact table size: 265,256 records\n",
      "Consensus outliers in sample: 542\n",
      "\n",
      "Records to remove: 542 (0.20%)\n",
      "Cleaned fact table size: 264,714 records\n",
      "\n",
      "Method Contributions to Removed Outliers:\n",
      "  Hampel X84 (statistics):      525 ( 96.9%)\n",
      "  Z-score (statistics):           0 (  0.0%)\n",
      "  LOF (distance):               314 ( 57.9%)\n",
      "  Isolation Forest (distance):  345 ( 63.7%)\n",
      "  Mahalanobis (multivariate):   542 (100.0%)\n",
      "\n",
      "Agreement Distribution (among removed outliers):\n",
      "  Flagged by all 5 methods:    0 (  0.0%)\n",
      "  Flagged by 4 methods:      100 ( 18.5%)\n",
      "  Flagged by 3 methods:      442 ( 81.5%)\n",
      "\n",
      "======================================================================\n",
      " Consensus-based outlier removal complete\n",
      " Removed 542 outliers using 5-method consensus (3+ votes)\n",
      "======================================================================\n",
      "Records to remove: 542 (0.20%)\n",
      "Cleaned fact table size: 264,714 records\n",
      "\n",
      "Method Contributions to Removed Outliers:\n",
      "  Hampel X84 (statistics):      525 ( 96.9%)\n",
      "  Z-score (statistics):           0 (  0.0%)\n",
      "  LOF (distance):               314 ( 57.9%)\n",
      "  Isolation Forest (distance):  345 ( 63.7%)\n",
      "  Mahalanobis (multivariate):   542 (100.0%)\n",
      "\n",
      "Agreement Distribution (among removed outliers):\n",
      "  Flagged by all 5 methods:    0 (  0.0%)\n",
      "  Flagged by 4 methods:      100 ( 18.5%)\n",
      "  Flagged by 3 methods:      442 ( 81.5%)\n",
      "\n",
      "======================================================================\n",
      " Consensus-based outlier removal complete\n",
      " Removed 542 outliers using 5-method consensus (3+ votes)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Consensus-Based Outlier Removal Strategy\n",
    "\n",
    "Based on Chapter 2 multi-method analysis, remove outliers identified by consensus\n",
    "of multiple detection techniques (Hampel, Z-score, LOF, Isolation Forest, Mahalanobis).\n",
    "\n",
    "Strategy:\n",
    "- Use consensus from ALL 5 detection methods\n",
    "- Only remove records flagged by 3+ methods (majority vote)\n",
    "- Ensures high confidence in outlier identification\n",
    "- Preserves edge cases that may be genuine failure patterns\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONSENSUS-BASED OUTLIER REMOVAL\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Removing outliers identified by 3+ detection methods (out of 5)\")\n",
    "print()\n",
    "\n",
    "# Check if consensus outliers exist from previous analysis\n",
    "if 'consensus_outliers' not in locals() or 'sample_indices' not in locals():\n",
    "    print(\"Error: Consensus outliers not found.\")\n",
    "    print(\"Run the previous outlier detection cell first.\")\n",
    "    fact_table_clean = fact_table.copy()\n",
    "else:\n",
    "    print(f\"Original fact table size: {len(fact_table):,} records\")\n",
    "    print(f\"Consensus outliers in sample: {consensus_outliers.sum():,}\")\n",
    "    print()\n",
    "    \n",
    "    # Create a mask for the full dataset\n",
    "\n",
    "    outlier_mask = pd.Series(False, index=fact_table.index)\n",
    "    \n",
    "    # Mark the sampled consensus outliers\n",
    "    outlier_indices = sample_indices[consensus_outliers]\n",
    "    outlier_mask.loc[outlier_indices] = True\n",
    "    \n",
    "    # Remove outliers\n",
    "    fact_table_clean = fact_table[~outlier_mask].copy()\n",
    "    \n",
    "    print(f\"Records to remove: {outlier_mask.sum():,} ({outlier_mask.sum()/len(fact_table)*100:.2f}%)\")\n",
    "    print(f\"Cleaned fact table size: {len(fact_table_clean):,} records\")\n",
    "    print()\n",
    "    \n",
    "    # Show which methods contributed to removed outliers\n",
    "    print(\"Method Contributions to Removed Outliers:\")\n",
    "    \n",
    "    if consensus_outliers.sum() > 0:\n",
    "\n",
    "  \n",
    "        hampel_count = outliers_hampel_sample[consensus_outliers].sum()\n",
    "        zscore_count = outliers_zscore_sample[consensus_outliers].sum()\n",
    "        lof_count = outliers_lof[consensus_outliers].sum()\n",
    "        iso_count = outliers_iso[consensus_outliers].sum()\n",
    "        mahal_count = outliers_mahal[consensus_outliers].sum()\n",
    "        \n",
    "        total_removed = consensus_outliers.sum()\n",
    "        print(f\"  Hampel X84 (statistics):     {hampel_count:>4,} ({hampel_count/total_removed*100:>5.1f}%)\")\n",
    "        print(f\"  Z-score (statistics):        {zscore_count:>4,} ({zscore_count/total_removed*100:>5.1f}%)\")\n",
    "        print(f\"  LOF (distance):              {lof_count:>4,} ({lof_count/total_removed*100:>5.1f}%)\")\n",
    "        print(f\"  Isolation Forest (distance): {iso_count:>4,} ({iso_count/total_removed*100:>5.1f}%)\")\n",
    "        print(f\"  Mahalanobis (multivariate):  {mahal_count:>4,} ({mahal_count/total_removed*100:>5.1f}%)\")\n",
    "        print()\n",
    "    \n",
    "        votes_for_removed = outlier_votes[consensus_outliers]\n",
    "        print(\"Agreement Distribution (among removed outliers):\")\n",
    "        print(f\"  Flagged by all 5 methods: {(votes_for_removed == 5).sum():>4,} ({(votes_for_removed == 5).sum()/total_removed*100:>5.1f}%)\")\n",
    "        print(f\"  Flagged by 4 methods:     {(votes_for_removed == 4).sum():>4,} ({(votes_for_removed == 4).sum()/total_removed*100:>5.1f}%)\")\n",
    "        print(f\"  Flagged by 3 methods:     {(votes_for_removed == 3).sum():>4,} ({(votes_for_removed == 3).sum()/total_removed*100:>5.1f}%)\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(f\" Consensus-based outlier removal complete\")\n",
    "    print(f\" Removed {outlier_mask.sum():,} outliers using 5-method consensus (3+ votes)\")\n",
    "    print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9049141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/davidhavera/CS598_FDC/wandb/run-20251027_112432-gfcpfg0u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/djhavera/cmapss-data-curation/runs/gfcpfg0u?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">outlier-removal-20251027-112432</a></strong> to <a href='https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/djhavera/cmapss-data-curation/runs/gfcpfg0u?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">https://wandb.ai/djhavera/cmapss-data-curation/runs/gfcpfg0u?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logging outlier removal metrics to W&B...\n",
      " Using input artifact: fact_table_raw:latest\n",
      " Using input artifact: fact_table_raw:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Artifacts logged anonymously cannot be claimed and expire after 7 days.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artifact logged: fact_table_clean:v0\n",
      "   Input rows: 265,256\n",
      "   Output rows: 264,714\n",
      "   Outliers removed: 542\n",
      " Run URL: https://wandb.ai/djhavera/cmapss-data-curation/runs/gfcpfg0u?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>hampel_contrib</td><td>▁</td></tr><tr><td>input_rows</td><td>▁</td></tr><tr><td>iso_contrib</td><td>▁</td></tr><tr><td>lof_contrib</td><td>▁</td></tr><tr><td>mahal_contrib</td><td>▁</td></tr><tr><td>outliers_removed</td><td>▁</td></tr><tr><td>output_rows</td><td>▁</td></tr><tr><td>removal_percentage</td><td>▁</td></tr><tr><td>zscore_contrib</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>hampel_contrib</td><td>525</td></tr><tr><td>input_rows</td><td>265256</td></tr><tr><td>iso_contrib</td><td>345</td></tr><tr><td>lof_contrib</td><td>314</td></tr><tr><td>mahal_contrib</td><td>542</td></tr><tr><td>outliers_removed</td><td>542</td></tr><tr><td>output_rows</td><td>264714</td></tr><tr><td>removal_percentage</td><td>0.20433</td></tr><tr><td>zscore_contrib</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">outlier-removal-20251027-112432</strong> at: <a href='https://wandb.ai/djhavera/cmapss-data-curation/runs/gfcpfg0u?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">https://wandb.ai/djhavera/cmapss-data-curation/runs/gfcpfg0u?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72</a><br> View project at: <a href='https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251027_112432-gfcpfg0u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# M12: Log outlier removal process to W&B (Step 2 in lineage)\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "\n",
    "# Initialize W&B for outlier removal step\n",
    "outlier_run = wandb.init(\n",
    "    project=\"cmapss-data-curation\",\n",
    "    name=f\"outlier-removal-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    job_type=\"outlier-removal\",\n",
    "    config={\n",
    "        \"detection_methods\": 5,\n",
    "        \"methods\": [\"Hampel\", \"Z-score\", \"LOF\", \"Isolation Forest\", \"Mahalanobis\"],\n",
    "        \"consensus_threshold\": 3,\n",
    "        \"sample_size\": 50000,\n",
    "        \"module\": \"M6 - Data Quality\"\n",
    "    },\n",
    "    tags=[\"outlier-removal\", \"data-quality\", \"m6\"],\n",
    "    anonymous=\"allow\"\n",
    ")\n",
    "\n",
    "print(\"\\n Logging outlier removal metrics to W&B...\")\n",
    "\n",
    "# Use the raw artifact from previous step\n",
    "raw_artifact = outlier_run.use_artifact('fact_table_raw:latest', type='dataset')\n",
    "print(f\" Using input artifact: {raw_artifact.name}\")\n",
    "\n",
    "# Log outlier removal statistics\n",
    "outlier_run.log({\n",
    "    \"input_rows\": len(fact_table),\n",
    "    \"output_rows\": len(fact_table_clean),\n",
    "    \"outliers_removed\": len(fact_table) - len(fact_table_clean),\n",
    "    \"removal_percentage\": ((len(fact_table) - len(fact_table_clean)) / len(fact_table)) * 100,\n",
    "    \"hampel_contrib\": hampel_contrib,\n",
    "    \"zscore_contrib\": zscore_contrib,\n",
    "    \"lof_contrib\": lof_contrib,\n",
    "    \"iso_contrib\": iso_contrib,\n",
    "    \"mahal_contrib\": mahal_contrib\n",
    "})\n",
    "\n",
    "# Save cleaned fact_table temporarily\n",
    "fact_table_clean.to_csv(\"fact_table_clean.csv\", index=False)\n",
    "\n",
    "# Create artifact for cleaned data\n",
    "clean_artifact = wandb.Artifact(\n",
    "    name=\"fact_table_clean\",\n",
    "    type=\"dataset\",\n",
    "    description=\"CMAPSS fact table after consensus-based outlier removal (3/5 methods)\",\n",
    "    metadata={\n",
    "        \"rows\": len(fact_table_clean),\n",
    "        \"columns\": len(fact_table_clean.columns),\n",
    "        \"outliers_removed\": len(fact_table) - len(fact_table_clean),\n",
    "        \"removal_rate\": ((len(fact_table) - len(fact_table_clean)) / len(fact_table)) * 100,\n",
    "        \"detection_methods\": [\"Hampel\", \"Z-score\", \"LOF\", \"Isolation Forest\", \"Mahalanobis\"],\n",
    "        \"consensus_threshold\": 3,\n",
    "        \"module\": \"M6 - Data Quality\"\n",
    "    }\n",
    ")\n",
    "\n",
    "clean_artifact.add_file(\"fact_table_clean.csv\")\n",
    "outlier_run.log_artifact(clean_artifact)\n",
    "\n",
    "print(f\" Artifact logged: fact_table_clean:v0\")\n",
    "print(f\"   Input rows: {len(fact_table):,}\")\n",
    "print(f\"   Output rows: {len(fact_table_clean):,}\")\n",
    "print(f\"   Outliers removed: {len(fact_table) - len(fact_table_clean):,}\")\n",
    "print(f\" Run URL: {outlier_run.get_url()}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da19f8",
   "metadata": {},
   "source": [
    "## 3.2 Data Deduplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28831fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deduplication analysis complete: 264,714 records analyzed, 0 removed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# DEDUPLICATION PROCESSING (Silent - Results reported in next cell)\n",
    "# =============================================================================\n",
    "\n",
    "# Check if we have the cleaned fact table\n",
    "if 'fact_table_clean' in locals():\n",
    "    df = fact_table_clean.copy()\n",
    "    \n",
    "    # Define all sensor columns\n",
    "    sensor_cols = [f'sensor_{i}' for i in range(1, 22)]  # sensor_1 to sensor_21\n",
    "    available_sensors = [col for col in sensor_cols if col in df.columns]\n",
    "    \n",
    "    # -------------------------\n",
    "    # 1. EXACT MATCH DEDUPLICATION\n",
    "    # -------------------------\n",
    "    # Find records that are identical across all sensor values\n",
    "    exact_duplicates = df.duplicated(subset=available_sensors, keep=False)\n",
    "    exact_dup_count = exact_duplicates.sum()\n",
    "    \n",
    "    if exact_dup_count > 0:\n",
    "        # Group identical sensor records\n",
    "        dup_records = df[exact_duplicates].copy()\n",
    "        sensor_groups = dup_records.groupby(available_sensors).size().sort_values(ascending=False)\n",
    "    \n",
    "    # -------------------------\n",
    "    # 2. ENTITY RESOLUTION (Unit + Cycle Duplicates)\n",
    "    # -------------------------\n",
    "    entity_dup_count = 0\n",
    "    if 'unit_id' in df.columns and 'time_cycles' in df.columns:\n",
    "        # Check for duplicate (unit_id, time_cycles) combinations\n",
    "        entity_duplicates = df.duplicated(subset=['unit_id', 'time_cycles'], keep=False)\n",
    "        entity_dup_count = entity_duplicates.sum()\n",
    "        \n",
    "        if entity_dup_count > 0:\n",
    "            dup_entities = df[entity_duplicates].groupby(['unit_id', 'time_cycles']).size().sort_values(ascending=False)\n",
    "    \n",
    "\n",
    "    # -------------------------\n",
    "    # 3. ANALYSIS COMPLETE - NO REMOVAL PERFORMED\n",
    "    # -------------------------\n",
    "    # This cell only analyzes for duplicates; it does NOT remove them\n",
    "    # All records are preserved in the output dataset\n",
    "    \n",
    "    # Save result (no records removed - this is analysis only)\n",
    "    fact_table_sensor_deduplicated = df.copy()\n",
    "    \n",
    "    print(f\" Deduplication analysis complete: {len(df):,} records analyzed, 0 removed\")\n",
    "    \n",
    "else:\n",
    "    print(\" fact_table_clean not found. Please run data cleaning steps first.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f195da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SENSOR DEDUPLICATION ANALYSIS & RESULTS\n",
      "================================================================================\n",
      "\n",
      " OVERALL RESULTS\n",
      "--------------------------------------------------------------------------------\n",
      "Input dataset (fact_table_clean):           264,714 records\n",
      "Output dataset (deduplicated):              264,714 records\n",
      "Records removed:                                  0 (0.00%)\n",
      "\n",
      " DEDUPLICATION METHODS APPLIED\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Exact Match Deduplication\n",
      "   Records with identical sensor values: 0\n",
      " No exact duplicates found\n",
      "\n",
      "2. Entity Resolution (unit_id + time_cycles)\n",
      "   Duplicate (unit_id, time_cycles) pairs: 247,672\n",
      " No entity duplicates found\n",
      "\n",
      " SENSOR DATA QUALITY INSIGHTS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Total sensors analyzed: 21\n",
      "\n",
      "Constant/near-constant sensors (≤5 unique values):\n",
      "  • sensor_16: 2 unique values\n",
      "  • sensor_19: 2 unique values\n",
      "\n",
      "High-duplicate sensors (>95% duplicate values):\n",
      "  • sensor_16: 100.0% duplicates (2 unique values)\n",
      "  • sensor_19: 100.0% duplicates (2 unique values)\n",
      "  • sensor_1: 100.0% duplicates (6 unique values)\n",
      "  • sensor_5: 100.0% duplicates (6 unique values)\n",
      "  • sensor_18: 100.0% duplicates (6 unique values)\n",
      "  • sensor_10: 100.0% duplicates (21 unique values)\n",
      "  • sensor_17: 100.0% duplicates (59 unique values)\n",
      "  • sensor_6: 100.0% duplicates (69 unique values)\n",
      "  • sensor_13: 99.8% duplicates (570 unique values)\n",
      "  • sensor_20: 99.7% duplicates (677 unique values)\n",
      "\n",
      "================================================================================\n",
      " DEDUPLICATION COMPLETE\n",
      "================================================================================\n",
      "Final dataset: 'fact_table_sensor_deduplicated' with 264,714 records\n",
      "Data quality: No duplicates removed - data already clean\n",
      "================================================================================\n",
      "\n",
      "High-duplicate sensors (>95% duplicate values):\n",
      "  • sensor_16: 100.0% duplicates (2 unique values)\n",
      "  • sensor_19: 100.0% duplicates (2 unique values)\n",
      "  • sensor_1: 100.0% duplicates (6 unique values)\n",
      "  • sensor_5: 100.0% duplicates (6 unique values)\n",
      "  • sensor_18: 100.0% duplicates (6 unique values)\n",
      "  • sensor_10: 100.0% duplicates (21 unique values)\n",
      "  • sensor_17: 100.0% duplicates (59 unique values)\n",
      "  • sensor_6: 100.0% duplicates (69 unique values)\n",
      "  • sensor_13: 99.8% duplicates (570 unique values)\n",
      "  • sensor_20: 99.7% duplicates (677 unique values)\n",
      "\n",
      "================================================================================\n",
      " DEDUPLICATION COMPLETE\n",
      "================================================================================\n",
      "Final dataset: 'fact_table_sensor_deduplicated' with 264,714 records\n",
      "Data quality: No duplicates removed - data already clean\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEDUPLICATION ANALYSIS REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SENSOR DEDUPLICATION ANALYSIS & RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'fact_table_sensor_deduplicated' not in locals():\n",
    "    print(\"\\n Error: Deduplication not completed. Run the previous cell first.\\n\")\n",
    "else:\n",
    "    # Overall Results\n",
    "    original_size = len(fact_table_clean) if 'fact_table_clean' in locals() else 0\n",
    "    cleaned_size = len(fact_table_sensor_deduplicated)\n",
    "    removed_count = original_size - cleaned_size\n",
    "    \n",
    "    print(f\"\\n OVERALL RESULTS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Input dataset (fact_table_clean):        {original_size:>10,} records\")\n",
    "    print(f\"Output dataset (deduplicated):           {cleaned_size:>10,} records\")\n",
    "    print(f\"Records removed:                         {removed_count:>10,} ({removed_count/original_size*100:.2f}%)\")\n",
    "    \n",
    "    # Analysis Methods Summary\n",
    "    print(f\"\\n DEDUPLICATION METHODS APPLIED\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Method 1: Exact Match\n",
    "    print(f\"\\n1. Exact Match Deduplication\")\n",
    "    if 'exact_dup_count' in locals():\n",
    "        print(f\"   Records with identical sensor values: {exact_dup_count:,}\")\n",
    "        if exact_dup_count == 0:\n",
    "            print(f\" No exact duplicates found\")\n",
    "        else:\n",
    "            print(f\"Removed {exact_dup_count:,} exact duplicate records\")\n",
    "    \n",
    "    # Method 2: Entity Resolution\n",
    "    print(f\"\\n2. Entity Resolution (unit_id + time_cycles)\")\n",
    "    if 'entity_dup_count' in locals():\n",
    "        print(f\"   Duplicate (unit_id, time_cycles) pairs: {entity_dup_count:,}\")\n",
    "        print(f\" No entity duplicates found\")\n",
    "\n",
    "    else:\n",
    "        print(f\" Skipped: unit_id or time_cycles columns not found\")\n",
    "    \n",
    "    # Sensor Data Quality Insights\n",
    "    print(f\"\\n SENSOR DATA QUALITY INSIGHTS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sensor_cols_list = [f'sensor_{i}' for i in range(1, 22)]\n",
    "    available_sensors_list = [col for col in sensor_cols_list if col in fact_table_clean.columns]\n",
    "    \n",
    "    print(f\"\\nTotal sensors analyzed: {len(available_sensors_list)}\")\n",
    "    \n",
    "    # Check for constant sensors\n",
    "    constant_sensors = []\n",
    "    for sensor in available_sensors_list:\n",
    "        if fact_table_clean[sensor].nunique() <= 5:\n",
    "            constant_sensors.append((sensor, fact_table_clean[sensor].nunique()))\n",
    "    \n",
    "    if constant_sensors:\n",
    "        print(f\"\\nConstant/near-constant sensors (≤5 unique values):\")\n",
    "        for sensor, n_unique in constant_sensors:\n",
    "            print(f\"  • {sensor}: {n_unique} unique values\")\n",
    "    else:\n",
    "        print(f\"\\n✓ No constant sensors detected\")\n",
    "    \n",
    "    # Check duplicate ratios for each sensor\n",
    "    high_dup_sensors = []\n",
    "    for sensor in available_sensors_list:\n",
    "        values = fact_table_clean[sensor]\n",
    "        dup_ratio = (len(values) - values.nunique()) / len(values)\n",
    "        if dup_ratio > 0.95:  # More than 95% duplicates\n",
    "            high_dup_sensors.append((sensor, dup_ratio, values.nunique()))\n",
    "    \n",
    "    if high_dup_sensors:\n",
    "        print(f\"\\nHigh-duplicate sensors (>95% duplicate values):\")\n",
    "        for sensor, ratio, n_unique in sorted(high_dup_sensors, key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"  • {sensor}: {ratio:.1%} duplicates ({n_unique:,} unique values)\")\n",
    "    else:\n",
    "        print(f\"\\n✓ No high-duplicate sensors detected\")\n",
    "    \n",
    "    # Final Summary\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\" DEDUPLICATION COMPLETE\")\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"Final dataset: 'fact_table_sensor_deduplicated' with {cleaned_size:,} records\")\n",
    "    print(f\"Data quality: {'No duplicates removed - data already clean' if removed_count == 0 else f'{removed_count:,} duplicate records removed'}\")\n",
    "    print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01aa08e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/davidhavera/CS598_FDC/wandb/run-20251027_112443-rogmaq0o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/djhavera/cmapss-data-curation/runs/rogmaq0o?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">feature-engineering-20251027-112443</a></strong> to <a href='https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/djhavera/cmapss-data-curation/runs/rogmaq0o?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">https://wandb.ai/djhavera/cmapss-data-curation/runs/rogmaq0o?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using input artifact: fact_table_clean:latest\n",
      " W&B initialized\n",
      " Dashboard: https://wandb.ai/djhavera/cmapss-data-curation/runs/rogmaq0o?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize W&B project for CMAPSS data curation (Step 3 in lineage)\n",
    "# Note: Set anonymous=\"allow\" for public visibility (academic transparency)\n",
    "feature_run = wandb.init(\n",
    "    project=\"cmapss-data-curation\",\n",
    "    name=f\"feature-engineering-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    job_type=\"feature-engineering\",\n",
    "    config={\n",
    "        # Transformation parameters\n",
    "        \"scaler_type\": \"StandardScaler\",\n",
    "        \"normalization_method\": \"z-score\",\n",
    "        \"num_sensors\": 21,\n",
    "        \"num_derived_features\": 4,\n",
    "        \n",
    "        # Feature definitions\n",
    "        \"thermal_index_sensors\": [\"sensor_2\", \"sensor_3\", \"sensor_4\"],\n",
    "        \"pressure_index_sensors\": [\"sensor_7\", \"sensor_8\", \"sensor_9\"],\n",
    "        \n",
    "        # Data quality\n",
    "        \"input_dataset\": \"fact_table_sensor_deduplicated\",\n",
    "        \"output_dataset\": \"fact_table_transformed\",\n",
    "        \n",
    "        # Module context\n",
    "        \"module\": \"M12 - Reproducibility & Transparency\",\n",
    "        \"transformation_phases\": [\"syntactic_normalization\", \"semantic_feature_engineering\"]\n",
    "    },\n",
    "    tags=[\"feature-engineering\", \"transformation\", \"m12-reproducibility\"],\n",
    "    anonymous=\"allow\"  # Makes this run publicly viewable for academic transparency\n",
    ")\n",
    "\n",
    "# Use the cleaned artifact from outlier removal step\n",
    "clean_artifact = feature_run.use_artifact('fact_table_clean:latest', type='dataset')\n",
    "print(f\" Using input artifact: {clean_artifact.name}\")\n",
    "print(f\" W&B initialized\")\n",
    "print(f\" Dashboard: {wandb.run.get_url()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c15373d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_transformation_pipeline_with_wandb(df):\n",
    "    \"\"\"\n",
    "    Execute feature engineering transformation with W&B tracking for M12 reproducibility.\n",
    "    \n",
    "    Tracks:\n",
    "    - Input data statistics\n",
    "    - Transformation parameters\n",
    "    - Output data statistics  \n",
    "    - Feature distributions\n",
    "    - Data quality metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # ============================================\n",
    "    # PHASE 0: Log Input Statistics\n",
    "    # ============================================\n",
    "    \n",
    "    print(\" Logging input data statistics...\")\n",
    "    \n",
    "    input_stats = {\n",
    "        \"input/rows\": len(df),\n",
    "        \"input/columns\": len(df.columns),\n",
    "        \"input/null_count\": df.isnull().sum().sum(),\n",
    "        \"input/null_percentage\": (df.isnull().sum().sum() / df.size) * 100,\n",
    "        \"input/memory_mb\": df.memory_usage(deep=True).sum() / 1e6\n",
    "    }\n",
    "    \n",
    "    wandb.log(input_stats)\n",
    "    \n",
    "    # Log sensor distributions (before transformation)\n",
    "    sensor_cols = [col for col in df.columns if col.startswith('sensor_')]\n",
    "    \n",
    "    sensor_stats_before = {}\n",
    "    for sensor in sensor_cols:\n",
    "        sensor_stats_before[f\"input_distribution/{sensor}_mean\"] = df[sensor].mean()\n",
    "        sensor_stats_before[f\"input_distribution/{sensor}_std\"] = df[sensor].std()\n",
    "        sensor_stats_before[f\"input_distribution/{sensor}_min\"] = df[sensor].min()\n",
    "        sensor_stats_before[f\"input_distribution/{sensor}_max\"] = df[sensor].max()\n",
    "    \n",
    "    wandb.log(sensor_stats_before)\n",
    "    \n",
    "    # ============================================\n",
    "    # PHASE 1: SYNTACTIC TRANSFORMATION (Z-score Normalization)\n",
    "    # ============================================\n",
    "    \n",
    "    print(\"Phase 1: Syntactic transformation (z-score normalization)...\")\n",
    "    \n",
    "    transformed_df = df.copy()\n",
    "    \n",
    "    # Apply StandardScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    transformed_df[sensor_cols] = scaler.fit_transform(df[sensor_cols])\n",
    "    \n",
    "    # Log scaler parameters\n",
    "    scaler_params = {\n",
    "        \"scaler/num_features\": len(sensor_cols),\n",
    "        \"scaler/mean_of_means\": scaler.mean_.mean(),\n",
    "        \"scaler/mean_of_stds\": scaler.scale_.mean()\n",
    "    }\n",
    "    wandb.log(scaler_params)\n",
    "    \n",
    "    # ============================================\n",
    "    # PHASE 2: SEMANTIC TRANSFORMATION (Derived Features)\n",
    "    # ============================================\n",
    "    \n",
    "    print(\" Phase 2: Semantic transformation (derived features)...\")\n",
    "    \n",
    "    # Temperature index\n",
    "    temp_sensors = ['sensor_2', 'sensor_3', 'sensor_4']\n",
    "    transformed_df['thermal_index'] = transformed_df[temp_sensors].mean(axis=1)\n",
    "    \n",
    "    # Pressure index\n",
    "    pressure_sensors = ['sensor_7', 'sensor_8', 'sensor_9']\n",
    "    transformed_df['pressure_index'] = transformed_df[pressure_sensors].mean(axis=1)\n",
    "    \n",
    "    # Normalized cycle (use correct column names: unit_id and time_cycles)\n",
    "    cycle_ranges = transformed_df.groupby('unit_id')['time_cycles'].agg(['min', 'max'])\n",
    "    transformed_df['normalized_cycle'] = transformed_df.apply(\n",
    "        lambda row: (row['time_cycles'] - cycle_ranges.loc[row['unit_id'], 'min']) / \n",
    "                    (cycle_ranges.loc[row['unit_id'], 'max'] - cycle_ranges.loc[row['unit_id'], 'min'] + 1),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Operational complexity (use correct column names: op_setting_1 and op_setting_2)\n",
    "    transformed_df['operational_complexity'] = (\n",
    "        transformed_df['op_setting_1'].abs() + \n",
    "        transformed_df['op_setting_2'].abs()\n",
    "    )\n",
    "    \n",
    "    # ============================================\n",
    "    # PHASE 3: Log Output Statistics\n",
    "    # ============================================\n",
    "    \n",
    "    print(\" Logging output data statistics...\")\n",
    "    \n",
    "    output_stats = {\n",
    "        \"output/rows\": len(transformed_df),\n",
    "        \"output/columns\": len(transformed_df.columns),\n",
    "        \"output/null_count\": transformed_df.isnull().sum().sum(),\n",
    "        \"output/null_percentage\": (transformed_df.isnull().sum().sum() / transformed_df.size) * 100,\n",
    "        \"output/memory_mb\": transformed_df.memory_usage(deep=True).sum() / 1e6,\n",
    "        \"output/new_features\": 4\n",
    "    }\n",
    "    \n",
    "    wandb.log(output_stats)\n",
    "    \n",
    "    # Log derived feature distributions\n",
    "    derived_features = ['thermal_index', 'pressure_index', 'normalized_cycle', 'operational_complexity']\n",
    "    \n",
    "    derived_stats = {}\n",
    "    for feature in derived_features:\n",
    "        derived_stats[f\"derived_features/{feature}_mean\"] = transformed_df[feature].mean()\n",
    "        derived_stats[f\"derived_features/{feature}_std\"] = transformed_df[feature].std()\n",
    "        derived_stats[f\"derived_features/{feature}_min\"] = transformed_df[feature].min()\n",
    "        derived_stats[f\"derived_features/{feature}_max\"] = transformed_df[feature].max()\n",
    "    \n",
    "    wandb.log(derived_stats)\n",
    "    \n",
    "    # ============================================\n",
    "    # PHASE 4: Create W&B Artifacts\n",
    "    # ============================================\n",
    "    \n",
    "    print(\" Creating W&B artifact...\")\n",
    "    \n",
    "    # Save transformed DataFrame to CSV for artifact\n",
    "    transformed_csv_path = \"fact_table_transformed.csv\"\n",
    "    transformed_df.to_csv(transformed_csv_path, index=False)\n",
    "    \n",
    "    # Create and log artifact\n",
    "    artifact = wandb.Artifact(\n",
    "        name=\"fact_table_transformed\",\n",
    "        type=\"dataset\",\n",
    "        description=\"Transformed CMAPSS dataset with z-score normalization and derived features\",\n",
    "        metadata={\n",
    "            \"rows\": len(transformed_df),\n",
    "            \"columns\": len(transformed_df.columns),\n",
    "            \"transformation_phases\": 2,\n",
    "            \"derived_features\": derived_features,\n",
    "            \"module\": \"M12 - Reproducibility & Transparency\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    artifact.add_file(transformed_csv_path)\n",
    "    wandb.log_artifact(artifact)\n",
    "    \n",
    "    print(\"Transformation complete with W&B tracking\")\n",
    "    print(f\" View results: {wandb.run.get_url()}\")\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "865971ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting feature engineering transformation with W&B tracking...\n",
      "\n",
      " Logging input data statistics...\n",
      "Phase 1: Syntactic transformation (z-score normalization)...\n",
      " Phase 2: Semantic transformation (derived features)...\n",
      "Phase 1: Syntactic transformation (z-score normalization)...\n",
      " Phase 2: Semantic transformation (derived features)...\n",
      " Logging output data statistics...\n",
      " Logging output data statistics...\n",
      " Creating W&B artifact...\n",
      " Creating W&B artifact...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Artifacts logged anonymously cannot be claimed and expire after 7 days.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation complete with W&B tracking\n",
      " View results: https://wandb.ai/djhavera/cmapss-data-curation/runs/rogmaq0o?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72\n",
      "\n",
      "================================================================================\n",
      "TRANSFORMATION SUMMARY (M12: Reproducibility & Transparency)\n",
      "================================================================================\n",
      "Input rows:      264,714\n",
      "Output rows:     264,714\n",
      "Input columns:   32\n",
      "Output columns:  36 (+4 derived features)\n",
      "\n",
      "Derived Features:\n",
      "  1. thermal_index - Mean of temperature sensors (sensor_2, 3, 4)\n",
      "  2. pressure_index - Mean of pressure sensors (sensor_7, 8, 9)\n",
      "  3. normalized_cycle - Cycle position scaled [0,1] per engine unit\n",
      "  4. operational_complexity - Combined operational settings\n",
      "\n",
      " W&B Dashboard: https://wandb.ai/djhavera/cmapss-data-curation/runs/rogmaq0o?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>derived_features/normalized_cycle_max</td><td>▁</td></tr><tr><td>derived_features/normalized_cycle_mean</td><td>▁</td></tr><tr><td>derived_features/normalized_cycle_min</td><td>▁</td></tr><tr><td>derived_features/normalized_cycle_std</td><td>▁</td></tr><tr><td>derived_features/operational_complexity_max</td><td>▁</td></tr><tr><td>derived_features/operational_complexity_mean</td><td>▁</td></tr><tr><td>derived_features/operational_complexity_min</td><td>▁</td></tr><tr><td>derived_features/operational_complexity_std</td><td>▁</td></tr><tr><td>derived_features/pressure_index_max</td><td>▁</td></tr><tr><td>derived_features/pressure_index_mean</td><td>▁</td></tr><tr><td>+104</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>derived_features/normalized_cycle_max</td><td>0.99816</td></tr><tr><td>derived_features/normalized_cycle_mean</td><td>0.37082</td></tr><tr><td>derived_features/normalized_cycle_min</td><td>0</td></tr><tr><td>derived_features/normalized_cycle_std</td><td>0.25342</td></tr><tr><td>derived_features/operational_complexity_max</td><td>42.85</td></tr><tr><td>derived_features/operational_complexity_mean</td><td>17.6177</td></tr><tr><td>derived_features/operational_complexity_min</td><td>0</td></tr><tr><td>derived_features/operational_complexity_std</td><td>16.88877</td></tr><tr><td>derived_features/pressure_index_max</td><td>1.14274</td></tr><tr><td>derived_features/pressure_index_mean</td><td>-0.0</td></tr><tr><td>+104</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feature-engineering-20251027-112443</strong> at: <a href='https://wandb.ai/djhavera/cmapss-data-curation/runs/rogmaq0o?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">https://wandb.ai/djhavera/cmapss-data-curation/runs/rogmaq0o?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72</a><br> View project at: <a href='https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72' target=\"_blank\">https://wandb.ai/djhavera/cmapss-data-curation?apiKey=ea47f4dba06a641a4c246b9c49cb570c0894cd72</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251027_112443-rogmaq0o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute transformation with W&B tracking\n",
    "print(\" Starting feature engineering transformation with W&B tracking...\")\n",
    "print()\n",
    "\n",
    "fact_table_transformed = execute_transformation_pipeline_with_wandb(fact_table_sensor_deduplicated)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"TRANSFORMATION SUMMARY (M12: Reproducibility & Transparency)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Input rows:      {len(fact_table_sensor_deduplicated):,}\")\n",
    "print(f\"Output rows:     {len(fact_table_transformed):,}\")\n",
    "print(f\"Input columns:   {len(fact_table_sensor_deduplicated.columns)}\")\n",
    "print(f\"Output columns:  {len(fact_table_transformed.columns)} (+4 derived features)\")\n",
    "print()\n",
    "print(\"Derived Features:\")\n",
    "print(\"  1. thermal_index - Mean of temperature sensors (sensor_2, 3, 4)\")\n",
    "print(\"  2. pressure_index - Mean of pressure sensors (sensor_7, 8, 9)\")\n",
    "print(\"  3. normalized_cycle - Cycle position scaled [0,1] per engine unit\")\n",
    "print(\"  4. operational_complexity - Combined operational settings\")\n",
    "print()\n",
    "print(f\" W&B Dashboard: {wandb.run.get_url()}\")\n",
    "wandb.finish()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5154cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.13/site-packages (1.16.2)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.13/site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in ./.venv/lib/python3.13/site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in ./.venv/lib/python3.13/site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for data transformation analysis\n",
    "%pip install scikit-learn scipy matplotlib seaborn\n",
    "# Initialize scalers\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scipy import stats as scipy_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b1219",
   "metadata": {},
   "source": [
    "## 3.3 Data Version Control (M9, M15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f8838",
   "metadata": {},
   "source": [
    "### DVC Workflow for Dataset Identity Tracking\n",
    "\n",
    "#### Step 1: Initialize DVC in Project\n",
    "\n",
    "```bash\n",
    "# Initialize DVC (creates .dvc/ directory)\n",
    "dvc init\n",
    "\n",
    "# DVC stores metadata in .dvc/config\n",
    "# Actual data hashes stored in .dvc/cache\n",
    "```\n",
    "\n",
    "#### Step 2: Track DataFrames with DVC\n",
    "\n",
    "**Scenario**: Track our fact table versions through ETL pipeline stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9846d690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "M9: EXPORTING DATAFRAMES FOR DVC TRACKING\n",
      "====================================================================================================\n",
      "\n",
      " Exported: fact_table.csv\n",
      "   Path: /Users/davidhavera/CS598_FDC/fact_table.csv\n",
      "   Size: 56.83 MB\n",
      "   Rows: 265,256\n",
      "   Columns: 32\n",
      "\n",
      " Exported: fact_table.csv\n",
      "   Path: /Users/davidhavera/CS598_FDC/fact_table.csv\n",
      "   Size: 56.83 MB\n",
      "   Rows: 265,256\n",
      "   Columns: 32\n",
      "\n",
      " Exported: fact_table_clean.csv\n",
      "   Path: /Users/davidhavera/CS598_FDC/fact_table_clean.csv\n",
      "   Size: 56.72 MB\n",
      "   Rows: 264,714\n",
      "   Columns: 32\n",
      "\n",
      " Exported: fact_table_clean.csv\n",
      "   Path: /Users/davidhavera/CS598_FDC/fact_table_clean.csv\n",
      "   Size: 56.72 MB\n",
      "   Rows: 264,714\n",
      "   Columns: 32\n",
      "\n",
      " Exported: fact_table_sensor_deduplicated.csv\n",
      "   Path: /Users/davidhavera/CS598_FDC/fact_table_sensor_deduplicated.csv\n",
      "   Size: 56.72 MB\n",
      "   Rows: 264,714\n",
      "   Columns: 32\n",
      "\n",
      " Exported: fact_table_sensor_deduplicated.csv\n",
      "   Path: /Users/davidhavera/CS598_FDC/fact_table_sensor_deduplicated.csv\n",
      "   Size: 56.72 MB\n",
      "   Rows: 264,714\n",
      "   Columns: 32\n",
      "\n",
      " Exported: fact_table_transformed.csv\n",
      "   Path: /Users/davidhavera/CS598_FDC/fact_table_transformed.csv\n",
      "   Size: 142.65 MB\n",
      "   Rows: 264,714\n",
      "   Columns: 36\n",
      "\n",
      "====================================================================================================\n",
      " SUMMARY: All 4 pipeline stage DataFrames exported successfully!\n",
      "====================================================================================================\n",
      "\n",
      " Files ready for DVC tracking:\n",
      "   1. fact_table.csv\n",
      "   2. fact_table_clean.csv\n",
      "   3. fact_table_sensor_deduplicated.csv\n",
      "   4. fact_table_transformed.csv\n",
      "\n",
      " Exported: fact_table_transformed.csv\n",
      "   Path: /Users/davidhavera/CS598_FDC/fact_table_transformed.csv\n",
      "   Size: 142.65 MB\n",
      "   Rows: 264,714\n",
      "   Columns: 36\n",
      "\n",
      "====================================================================================================\n",
      " SUMMARY: All 4 pipeline stage DataFrames exported successfully!\n",
      "====================================================================================================\n",
      "\n",
      " Files ready for DVC tracking:\n",
      "   1. fact_table.csv\n",
      "   2. fact_table_clean.csv\n",
      "   3. fact_table_sensor_deduplicated.csv\n",
      "   4. fact_table_transformed.csv\n"
     ]
    }
   ],
   "source": [
    "# M9: Export all pipeline stage DataFrames as CSV files for DVC tracking\n",
    "import os\n",
    "\n",
    "# Define output directory (current working directory)\n",
    "output_dir = os.getcwd()\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"M9: EXPORTING DATAFRAMES FOR DVC TRACKING\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Export Stage 1: fact_table\n",
    "csv_file_1 = os.path.join(output_dir, \"fact_table.csv\")\n",
    "fact_table.to_csv(csv_file_1, index=False)\n",
    "size_mb = os.path.getsize(csv_file_1) / 1024**2\n",
    "print(f\"\\n Exported: fact_table.csv\")\n",
    "print(f\"   Path: {csv_file_1}\")\n",
    "print(f\"   Size: {size_mb:.2f} MB\")\n",
    "print(f\"   Rows: {fact_table.shape[0]:,}\")\n",
    "print(f\"   Columns: {fact_table.shape[1]}\")\n",
    "\n",
    "# Export Stage 2: fact_table_clean\n",
    "csv_file_2 = os.path.join(output_dir, \"fact_table_clean.csv\")\n",
    "fact_table_clean.to_csv(csv_file_2, index=False)\n",
    "size_mb = os.path.getsize(csv_file_2) / 1024**2\n",
    "print(f\"\\n Exported: fact_table_clean.csv\")\n",
    "print(f\"   Path: {csv_file_2}\")\n",
    "print(f\"   Size: {size_mb:.2f} MB\")\n",
    "print(f\"   Rows: {fact_table_clean.shape[0]:,}\")\n",
    "print(f\"   Columns: {fact_table_clean.shape[1]}\")\n",
    "\n",
    "# Export Stage 3: fact_table_sensor_deduplicated\n",
    "csv_file_3 = os.path.join(output_dir, \"fact_table_sensor_deduplicated.csv\")\n",
    "fact_table_sensor_deduplicated.to_csv(csv_file_3, index=False)\n",
    "size_mb = os.path.getsize(csv_file_3) / 1024**2\n",
    "print(f\"\\n Exported: fact_table_sensor_deduplicated.csv\")\n",
    "print(f\"   Path: {csv_file_3}\")\n",
    "print(f\"   Size: {size_mb:.2f} MB\")\n",
    "print(f\"   Rows: {fact_table_sensor_deduplicated.shape[0]:,}\")\n",
    "print(f\"   Columns: {fact_table_sensor_deduplicated.shape[1]}\")\n",
    "\n",
    "# Export Stage 4: fact_table_transformed\n",
    "csv_file_4 = os.path.join(output_dir, \"fact_table_transformed.csv\")\n",
    "fact_table_transformed.to_csv(csv_file_4, index=False)\n",
    "size_mb = os.path.getsize(csv_file_4) / 1024**2\n",
    "print(f\"\\n Exported: fact_table_transformed.csv\")\n",
    "print(f\"   Path: {csv_file_4}\")\n",
    "print(f\"   Size: {size_mb:.2f} MB\")\n",
    "print(f\"   Rows: {fact_table_transformed.shape[0]:,}\")\n",
    "print(f\"   Columns: {fact_table_transformed.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\" SUMMARY: All 4 pipeline stage DataFrames exported successfully!\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\n Files ready for DVC tracking:\")\n",
    "csv_files = [\n",
    "    \"fact_table.csv\",\n",
    "    \"fact_table_clean.csv\", \n",
    "    \"fact_table_sensor_deduplicated.csv\",\n",
    "    \"fact_table_transformed.csv\"\n",
    "]\n",
    "for i, filename in enumerate(csv_files, 1):\n",
    "    print(f\"   {i}. {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c453ffae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "M9: DVC TRACKING AND HASH VERIFICATION\n",
      "====================================================================================================\n",
      "\n",
      "Step 1: Initialize DVC repository\n",
      "----------------------------------------------------------------------------------------------------\n",
      " =DVC initialized successfully\n",
      "\n",
      "Step 2: Track CSV files with DVC\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "   Tracking: fact_table.csv\n",
      " =DVC initialized successfully\n",
      "\n",
      "Step 2: Track CSV files with DVC\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "   Tracking: fact_table.csv\n",
      " fact_table.csv tracked successfully\n",
      "      DVC MD5: 22231066cb43c5abf1e0c071e5026c3e\n",
      "\n",
      "   Tracking: fact_table_clean.csv\n",
      " fact_table.csv tracked successfully\n",
      "      DVC MD5: 22231066cb43c5abf1e0c071e5026c3e\n",
      "\n",
      "   Tracking: fact_table_clean.csv\n",
      " fact_table_clean.csv tracked successfully\n",
      "      DVC MD5: 9b250b9ba65bac3b39d81fb020873675\n",
      "\n",
      "   Tracking: fact_table_sensor_deduplicated.csv\n",
      " fact_table_clean.csv tracked successfully\n",
      "      DVC MD5: 9b250b9ba65bac3b39d81fb020873675\n",
      "\n",
      "   Tracking: fact_table_sensor_deduplicated.csv\n",
      " fact_table_sensor_deduplicated.csv tracked successfully\n",
      "      DVC MD5: 9b250b9ba65bac3b39d81fb020873675\n",
      "\n",
      "   Tracking: fact_table_transformed.csv\n",
      " fact_table_sensor_deduplicated.csv tracked successfully\n",
      "      DVC MD5: 9b250b9ba65bac3b39d81fb020873675\n",
      "\n",
      "   Tracking: fact_table_transformed.csv\n",
      " fact_table_transformed.csv tracked successfully\n",
      "      DVC MD5: 0bc30a0149f3218fd0e0524292874906\n",
      "\n",
      "====================================================================================================\n",
      "Step 3: Verify DVC Cache\n",
      "====================================================================================================\n",
      "\n",
      "   Cache directory: /Users/davidhavera/CS598_FDC/.dvc/cache\n",
      "   Cache size: 1252.46 MB\n",
      "   Files cached: 4\n",
      "\n",
      "====================================================================================================\n",
      "M9 DVC TRACKING COMPLETE\n",
      "====================================================================================================\n",
      "\n",
      "Tracked 4 CSV files with DVC:\n",
      "   1. fact_table.csv → fact_table.csv.dvc\n",
      "   2. fact_table_clean.csv → fact_table_clean.csv.dvc\n",
      "   3. fact_table_sensor_deduplicated.csv → fact_table_sensor_deduplicated.csv.dvc\n",
      "   4. fact_table_transformed.csv → fact_table_transformed.csv.dvc\n",
      "\n",
      " Next steps:\n",
      "   1. Commit .dvc files to Git: git add *.dvc .dvc/config\n",
      "   2. Add CSV files to .gitignore (already done by DVC)\n",
      "   3. Push to Git: git commit -m 'Track datasets with DVC'\n",
      "====================================================================================================\n",
      " fact_table_transformed.csv tracked successfully\n",
      "      DVC MD5: 0bc30a0149f3218fd0e0524292874906\n",
      "\n",
      "====================================================================================================\n",
      "Step 3: Verify DVC Cache\n",
      "====================================================================================================\n",
      "\n",
      "   Cache directory: /Users/davidhavera/CS598_FDC/.dvc/cache\n",
      "   Cache size: 1252.46 MB\n",
      "   Files cached: 4\n",
      "\n",
      "====================================================================================================\n",
      "M9 DVC TRACKING COMPLETE\n",
      "====================================================================================================\n",
      "\n",
      "Tracked 4 CSV files with DVC:\n",
      "   1. fact_table.csv → fact_table.csv.dvc\n",
      "   2. fact_table_clean.csv → fact_table_clean.csv.dvc\n",
      "   3. fact_table_sensor_deduplicated.csv → fact_table_sensor_deduplicated.csv.dvc\n",
      "   4. fact_table_transformed.csv → fact_table_transformed.csv.dvc\n",
      "\n",
      " Next steps:\n",
      "   1. Commit .dvc files to Git: git add *.dvc .dvc/config\n",
      "   2. Add CSV files to .gitignore (already done by DVC)\n",
      "   3. Push to Git: git commit -m 'Track datasets with DVC'\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# M9: Track CSV files with DVC and verify MD5 hashes\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"M9: DVC TRACKING AND HASH VERIFICATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Initialize DVC (if not already initialized)\n",
    "print(\"\\nStep 1: Initialize DVC repository\")\n",
    "print(\"-\" * 100)\n",
    "result = subprocess.run(['dvc', 'init'], capture_output=True, text=True, cwd=os.getcwd())\n",
    "if \"already exists\" in result.stderr:\n",
    "    print(\" DVC already initialized\")\n",
    "else:\n",
    "    print(\" =DVC initialized successfully\")\n",
    "\n",
    "# Track each CSV file with DVC\n",
    "csv_files = [\n",
    "    \"fact_table.csv\",\n",
    "    \"fact_table_clean.csv\",\n",
    "    \"fact_table_sensor_deduplicated.csv\", \n",
    "    \"fact_table_transformed.csv\"\n",
    "]\n",
    "\n",
    "tracked_files = []\n",
    "print(\"\\nStep 2: Track CSV files with DVC\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    print(f\"\\n   Tracking: {csv_file}\")\n",
    "    result = subprocess.run(['dvc', 'add', csv_file], capture_output=True, text=True, cwd=os.getcwd())\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\" {csv_file} tracked successfully\")\n",
    "        tracked_files.append(csv_file)\n",
    "        \n",
    "        # Read the .dvc file to get MD5 hash\n",
    "        dvc_file = f\"{csv_file}.dvc\"\n",
    "        if os.path.exists(dvc_file):\n",
    "            with open(dvc_file, 'r') as f:\n",
    "                dvc_content = f.read()\n",
    "                # Extract MD5 hash from .dvc file\n",
    "                for line in dvc_content.split('\\n'):\n",
    "                    if 'md5:' in line:\n",
    "                        dvc_hash = line.split('md5:')[1].strip()\n",
    "                        print(f\"      DVC MD5: {dvc_hash}\")\n",
    "    else:\n",
    "        print(f\"Error tracking {csv_file}: {result.stderr}\")\n",
    "\n",
    "# Verify DVC cache\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"Step 3: Verify DVC Cache\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "cache_dir = os.path.join(os.getcwd(), \".dvc\", \"cache\")\n",
    "if os.path.exists(cache_dir):\n",
    "    cache_size = sum(\n",
    "        os.path.getsize(os.path.join(dirpath, filename))\n",
    "        for dirpath, dirnames, filenames in os.walk(cache_dir)\n",
    "        for filename in filenames\n",
    "    )\n",
    "    print(f\"\\n   Cache directory: {cache_dir}\")\n",
    "    print(f\"   Cache size: {cache_size / 1024**2:.2f} MB\")\n",
    "    print(f\"   Files cached: {len(tracked_files)}\")\n",
    "else:\n",
    "    print(\"   Cache directory not found\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"M9 DVC TRACKING COMPLETE\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nTracked {len(tracked_files)} CSV files with DVC:\")\n",
    "for i, filename in enumerate(tracked_files, 1):\n",
    "    dvc_file = f\"{filename}.dvc\"\n",
    "    print(f\"   {i}. {filename} → {dvc_file}\")\n",
    "\n",
    "print(\"\\n Next steps:\")\n",
    "print(\"   1. Commit .dvc files to Git: git add *.dvc .dvc/config\")\n",
    "print(\"   2. Add CSV files to .gitignore (already done by DVC)\")\n",
    "print(\"   3. Push to Git: git commit -m 'Track datasets with DVC'\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a784072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
